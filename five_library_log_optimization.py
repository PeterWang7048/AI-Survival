"""
äº”åº“ç³»ç»Ÿæ—¥å¿—ä¼˜åŒ–æ–¹æ¡ˆ
è§£å†³ç»éªŒåŒæ­¥æ—¥å¿—è¿‡å¤šçš„é—®é¢˜
"""

import time
import json
import logging
from collections import defaultdict, deque
from typing import Dict, Any, Set
from threading import Lock

class OptimizedSyncLogger:
    """ä¼˜åŒ–çš„åŒæ­¥æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, aggregate_interval: int = 30, max_individual_logs: int = 5):
        """
        åˆå§‹åŒ–ä¼˜åŒ–æ—¥å¿—è®°å½•å™¨
        
        Args:
            aggregate_interval: èšåˆæ—¥å¿—çš„æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
            max_individual_logs: å•ä¸ªæ—¶é—´çª—å£å†…æœ€å¤§ä¸ªåˆ«æ—¥å¿—æ•°é‡
        """
        self.aggregate_interval = aggregate_interval
        self.max_individual_logs = max_individual_logs
        
        # æ—¥å¿—èšåˆæ•°æ®
        self.sync_stats = {
            'merged': 0,
            'created': 0,
            'updated': 0,
            'skipped': 0,
            'failed': 0
        }
        
        # é‡å¤æ—¥å¿—æ£€æµ‹
        self.recent_hashes = deque(maxlen=100)  # æœ€è¿‘100ä¸ªå“ˆå¸Œ
        self.hash_counts = defaultdict(int)
        
        # æ—¶é—´çª—å£ç®¡ç†
        self.last_aggregate_time = time.time()
        self.current_window_logs = 0
        
        # çº¿ç¨‹å®‰å…¨
        self.lock = Lock()
        
        # è·å–logger
        self.logger = logging.getLogger('five_library_system')
    
    def log_sync_result(self, sync_result: Dict[str, Any], content_hash: str):
        """
        è®°å½•åŒæ­¥ç»“æœï¼ˆæ™ºèƒ½å»é‡å’Œèšåˆï¼‰
        
        Args:
            sync_result: åŒæ­¥ç»“æœ
            content_hash: å†…å®¹å“ˆå¸Œ
        """
        with self.lock:
            action = sync_result.get('action', 'unknown')
            
            # 1. æ›´æ–°ç»Ÿè®¡
            if action in self.sync_stats:
                self.sync_stats[action] += 1
            
            # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºä¸ªåˆ«æ—¥å¿—
            should_log_individual = self._should_log_individual(content_hash, action)
            
            if should_log_individual:
                self._log_individual_sync(sync_result, content_hash)
                self.current_window_logs += 1
            
            # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºèšåˆæ—¥å¿—
            self._check_and_output_aggregate()
    
    def _should_log_individual(self, content_hash: str, action: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¾“å‡ºä¸ªåˆ«æ—¥å¿—"""
        # 1. å¦‚æœå½“å‰çª—å£å·²è¾¾åˆ°æœ€å¤§ä¸ªåˆ«æ—¥å¿—æ•°ï¼Œä¸è¾“å‡º
        if self.current_window_logs >= self.max_individual_logs:
            return False
        
        # 2. å¯¹äºfailedå’Œcreatedï¼Œæ€»æ˜¯è¾“å‡ºï¼ˆé‡è¦ä¿¡æ¯ï¼‰
        if action in ['failed', 'created']:
            return True
        
        # 3. å¯¹äºmergedå’Œupdatedï¼Œæ£€æŸ¥æ˜¯å¦é‡å¤
        hash_short = content_hash[:16]
        self.hash_counts[hash_short] += 1
        
        # ç¬¬ä¸€æ¬¡å‡ºç°æ€»æ˜¯è®°å½•
        if self.hash_counts[hash_short] == 1:
            self.recent_hashes.append(hash_short)
            return True
        
        # é‡å¤å‡ºç°ä½†é¢‘ç‡ä¸é«˜æ—¶è®°å½•ï¼ˆæ¯5æ¬¡è®°å½•ä¸€æ¬¡ï¼‰
        if self.hash_counts[hash_short] % 5 == 0:
            return True
        
        return False
    
    def _log_individual_sync(self, sync_result: Dict[str, Any], content_hash: str):
        """è¾“å‡ºä¸ªåˆ«åŒæ­¥æ—¥å¿—"""
        action = sync_result.get('action', 'unknown')
        hash_short = content_hash[:16]
        
        if action == 'failed':
            self.logger.error(f"âŒ Experience sync failed: {sync_result.get('reason', 'unknown error')} - {hash_short}...")
        elif action == 'created':
            self.logger.info(f"âœ¨ New experience created: {hash_short}... (discoverers: {sync_result.get('discoverer_count', 1)})")
        elif action == 'merged':
            count = self.hash_counts[hash_short]
            if count == 1:
                self.logger.debug(f"ğŸ”„ ç»éªŒåˆå¹¶: {hash_short}... (ç´¯è®¡: {sync_result.get('total_occurrence_count', 0)})")
            else:
                self.logger.debug(f"ğŸ”„ ç»éªŒåˆå¹¶: {hash_short}... (ç¬¬{count}æ¬¡, ç´¯è®¡: {sync_result.get('total_occurrence_count', 0)})")
    
    def _check_and_output_aggregate(self):
        """æ£€æŸ¥å¹¶è¾“å‡ºèšåˆæ—¥å¿—"""
        current_time = time.time()
        
        if current_time - self.last_aggregate_time >= self.aggregate_interval:
            self._output_aggregate_stats()
            self._reset_window()
    
    def _output_aggregate_stats(self):
        """è¾“å‡ºèšåˆç»Ÿè®¡æ—¥å¿—"""
        total_syncs = sum(self.sync_stats.values())
        
        if total_syncs == 0:
            return
        
        # æ„å»ºèšåˆæ¶ˆæ¯
        stats_parts = []
        for action, count in self.sync_stats.items():
            if count > 0:
                if action == 'merged':
                    stats_parts.append(f"åˆå¹¶{count}æ¡")
                elif action == 'created':
                    stats_parts.append(f"æ–°å»º{count}æ¡")
                elif action == 'updated':
                    stats_parts.append(f"æ›´æ–°{count}æ¡")
                elif action == 'failed':
                    stats_parts.append(f"å¤±è´¥{count}æ¡")
                elif action == 'skipped':
                    stats_parts.append(f"è·³è¿‡{count}æ¡")
        
        if stats_parts:
            message = f"ğŸ“Š ç»éªŒåŒæ­¥æ±‡æ€»({self.aggregate_interval}s): " + ", ".join(stats_parts)
            
            # æ ¹æ®å¤±è´¥æ•°é‡å†³å®šæ—¥å¿—çº§åˆ«
            if self.sync_stats['failed'] > 0:
                self.logger.warning(message)
            else:
                self.logger.info(message)
    
    def _reset_window(self):
        """é‡ç½®æ—¶é—´çª—å£"""
        self.last_aggregate_time = time.time()
        self.current_window_logs = 0
        
        # é‡ç½®ç»Ÿè®¡ï¼ˆä¿ç•™ä¸€äº›å†å²æ•°æ®ç”¨äºè¶‹åŠ¿åˆ†æï¼‰
        for key in self.sync_stats:
            self.sync_stats[key] = 0
        
        # æ¸…ç†è¿‡æœŸçš„å“ˆå¸Œè®¡æ•°
        self._cleanup_hash_counts()
    
    def _cleanup_hash_counts(self):
        """æ¸…ç†è¿‡æœŸçš„å“ˆå¸Œè®¡æ•°"""
        # åªä¿ç•™æœ€è¿‘çš„å“ˆå¸Œè®¡æ•°
        if len(self.hash_counts) > 200:  # é¿å…å†…å­˜æ³„éœ²
            # ä¿ç•™æœ€è¿‘ä½¿ç”¨çš„100ä¸ª
            recent_set = set(self.recent_hashes)
            keys_to_remove = [k for k in self.hash_counts.keys() if k not in recent_set]
            
            for key in keys_to_remove[:len(keys_to_remove)//2]:  # åˆ é™¤ä¸€åŠæ—§æ•°æ®
                del self.hash_counts[key]
    
    def force_aggregate_output(self):
        """å¼ºåˆ¶è¾“å‡ºå½“å‰èšåˆç»Ÿè®¡"""
        with self.lock:
            self._output_aggregate_stats()
            self._reset_window()
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                'current_window_stats': self.sync_stats.copy(),
                'unique_hashes_tracked': len(self.hash_counts),
                'recent_hashes_count': len(self.recent_hashes),
                'current_window_logs': self.current_window_logs,
                'time_since_last_aggregate': time.time() - self.last_aggregate_time
            }


class SmartLogFilter:
    """æ™ºèƒ½æ—¥å¿—è¿‡æ»¤å™¨"""
    
    def __init__(self):
        self.pattern_counts = defaultdict(int)
        self.suppression_thresholds = {
            'ç»éªŒåŒæ­¥æˆåŠŸ': 10,  # 10æ¬¡åå¼€å§‹æŠ‘åˆ¶
            'ç»éªŒå·²æ·»åŠ åˆ°ç›´æ¥ç»éªŒåº“': 15,
            'è§„å¾‹åŒæ­¥æˆåŠŸ': 8,
        }
        self.suppression_ratios = {
            'ç»éªŒåŒæ­¥æˆåŠŸ': 0.1,  # åªæ˜¾ç¤º10%
            'ç»éªŒå·²æ·»åŠ åˆ°ç›´æ¥ç»éªŒåº“': 0.05,  # åªæ˜¾ç¤º5%
            'è§„å¾‹åŒæ­¥æˆåŠŸ': 0.2,  # åªæ˜¾ç¤º20%
        }
    
    def should_log(self, message: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è®°å½•æ—¥å¿—"""
        # è¯†åˆ«æ¶ˆæ¯æ¨¡å¼
        pattern = self._identify_pattern(message)
        
        if pattern not in self.suppression_thresholds:
            return True  # æœªè¯†åˆ«çš„æ¨¡å¼æ­£å¸¸è®°å½•
        
        self.pattern_counts[pattern] += 1
        count = self.pattern_counts[pattern]
        
        # åœ¨é˜ˆå€¼ä¹‹å‰æ­£å¸¸è®°å½•
        if count <= self.suppression_thresholds[pattern]:
            return True
        
        # è¶…è¿‡é˜ˆå€¼åæŒ‰æ¯”ä¾‹æŠ‘åˆ¶
        ratio = self.suppression_ratios[pattern]
        return (count % int(1 / ratio)) == 0
    
    def _identify_pattern(self, message: str) -> str:
        """è¯†åˆ«æ—¥å¿—æ¶ˆæ¯çš„æ¨¡å¼"""
        for pattern in self.suppression_thresholds.keys():
            if pattern in message:
                return pattern
        return 'unknown'
    
    def get_suppression_stats(self) -> Dict[str, Dict[str, int]]:
        """è·å–æŠ‘åˆ¶ç»Ÿè®¡"""
        stats = {}
        for pattern, count in self.pattern_counts.items():
            if pattern in self.suppression_thresholds:
                threshold = self.suppression_thresholds[pattern]
                suppressed = max(0, count - threshold)
                ratio = self.suppression_ratios.get(pattern, 1.0)
                if count > threshold:
                    suppressed = int(suppressed * (1 - ratio))
                
                stats[pattern] = {
                    'total_occurrences': count,
                    'threshold': threshold,
                    'suppressed_count': suppressed,
                    'logged_count': count - suppressed,
                    'suppression_ratio': f"{(suppressed/count*100):.1f}%" if count > 0 else "0%"
                }
        return stats


def create_optimized_five_library_logger() -> OptimizedSyncLogger:
    """åˆ›å»ºä¼˜åŒ–çš„äº”åº“ç³»ç»Ÿæ—¥å¿—è®°å½•å™¨"""
    return OptimizedSyncLogger(
        aggregate_interval=30,  # 30ç§’èšåˆä¸€æ¬¡
        max_individual_logs=3   # æ¯ä¸ªçª—å£æœ€å¤š3æ¡ä¸ªåˆ«æ—¥å¿—
    )


def apply_log_optimization_to_five_library():
    """åº”ç”¨æ—¥å¿—ä¼˜åŒ–åˆ°äº”åº“ç³»ç»Ÿçš„ç¤ºä¾‹ä»£ç """
    
    # åˆ›å»ºä¼˜åŒ–çš„æ—¥å¿—è®°å½•å™¨
    sync_logger = create_optimized_five_library_logger()
    
    # åœ¨ sync_experience_to_total_library æ–¹æ³•ä¸­æ›¿æ¢åŸæœ‰æ—¥å¿—
    def optimized_sync_logging(sync_result, content_hash):
        """ä¼˜åŒ–çš„åŒæ­¥æ—¥å¿—è®°å½•"""
        # æ›¿æ¢åŸæ¥çš„ logger.info(f"âœ… ç»éªŒåŒæ­¥æˆåŠŸ: {sync_result['action']} - {content_hash[:16]}...")
        sync_logger.log_sync_result(sync_result, content_hash)
    
    return sync_logger


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    sync_logger = create_optimized_five_library_logger()
    
    # æ¨¡æ‹Ÿå¤§é‡åŒæ­¥æ“ä½œ
    print("ğŸ§ª æµ‹è¯•æ—¥å¿—ä¼˜åŒ–æ•ˆæœ...")
    
    # æ¨¡æ‹Ÿ100ä¸ªåŒæ­¥æ“ä½œ
    for i in range(100):
        sync_result = {
            'success': True,
            'action': 'merged' if i % 3 == 0 else 'created' if i % 10 == 0 else 'updated',
            'total_occurrence_count': i + 1,
            'discoverer_count': 1
        }
        content_hash = f"hash_{i % 20:032d}"  # æ¨¡æ‹Ÿä¸€äº›é‡å¤å“ˆå¸Œ
        
        sync_logger.log_sync_result(sync_result, content_hash)
        
        # æ¨¡æ‹Ÿæ—¶é—´æµé€
        if i % 30 == 29:
            time.sleep(0.1)  # å°å»¶è¿Ÿè§¦å‘èšåˆ
    
    # å¼ºåˆ¶è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    sync_logger.force_aggregate_output()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = sync_logger.get_statistics()
    print(f"\nğŸ“Š ä¼˜åŒ–æ•ˆæœç»Ÿè®¡: {stats}")
    
    print("\nâœ… æ—¥å¿—ä¼˜åŒ–æµ‹è¯•å®Œæˆ") 