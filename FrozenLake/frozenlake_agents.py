#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡å®éªŒæ™ºèƒ½ä½“é›†åˆ
==================

åŒ…å«æ‰€æœ‰6ä¸ªåŸºçº¿ç®—æ³•ï¼š
1. RegionBasedILAI - æœ€ä½³å•å‘ä¼˜åŒ–ç‰ˆæœ¬ (72-75%æˆåŠŸç‡)
2. ImprovedDQNAgent - æ”¹è¿›ç‰ˆæ·±åº¦Qç½‘ç»œ
3. ImprovedQLearningAgent - æ”¹è¿›ç‰ˆQå­¦ä¹ 
4. ProbabilisticAStarAgent - æ¦‚ç‡æ„ŸçŸ¥A*æœç´¢
5. SlipperyAwareRuleAgent - æ»‘åŠ¨æ„ŸçŸ¥è§„åˆ™æ™ºèƒ½ä½“
6. RandomAgent - éšæœºåŸºçº¿
"""

import time
import random
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict, deque
from frozenlake_experiment_manager import DecisionResult

# å¯¼å…¥æ”¹è¿›ç‰ˆåŸºçº¿ç®—æ³•
from improved_baselines import (
    ImprovedQLearningAgent,
    ImprovedDQNAgent, 
    SlipperyAwareRuleAgent,
    ProbabilisticAStarAgent
)

# ============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# ============================================================================

@dataclass
class RegionResult:
    """åŒºåŸŸåŒ–ç»“æœ"""
    position: int
    probability: float
    risk_score: float
    reward_potential: float

@dataclass
class ActionRegion:
    """åŠ¨ä½œç»“æœåŒºåŸŸ"""
    action: str
    intended_position: int
    possible_results: List[RegionResult]
    total_risk: float
    expected_reward: float
    safety_score: float

# ============================================================================
# 1. æœ€ä½³ILAIç³»ç»Ÿï¼šRegionBasedILAI (72-75%æˆåŠŸç‡ç‰ˆæœ¬)
# ============================================================================

class FairAcademicRegionILAI:
    """å­¦æœ¯å…¬å¹³çš„åŒºåŸŸåŒ–ILAIç³»ç»Ÿ - åŸºäºultra_optimized_user_brilliant_idea.py"""
    
    def __init__(self, player_name: str = "ILAIç³»ç»Ÿ_å­¦æœ¯å…¬å¹³ç‰ˆ"):
        self.player_name = player_name
        self.current_episode_id = None
        self.current_step = 0
        self.decision_log = []
        
        # ===================== è¶…çº§ä¼˜åŒ–åŸºç¡€é…ç½® =====================
        self.base_region_config = {
            'intended_action_prob': 1/3,
            'slip_prob_per_direction': 1/3,
            'slip_directions': 2,
            
            # ğŸš€ è¶…çº§ä¼˜åŒ–æƒé‡é…ç½®
            'safety_weight': 0.35,          # ä¼˜åŒ–ï¼š0.4 â†’ 0.35
            'progress_weight': 0.35,        # ä¼˜åŒ–ï¼š0.3 â†’ 0.35
            'exploration_weight': 0.25,     # ä¼˜åŒ–ï¼š0.2 â†’ 0.25
            'risk_aversion': 0.05,          # ä¼˜åŒ–ï¼š0.1 â†’ 0.05
            
            'high_risk_threshold': 0.65,    # ä¼˜åŒ–ï¼š0.6 â†’ 0.65
            'safe_region_threshold': 0.25,  # ä¼˜åŒ–ï¼š0.3 â†’ 0.25
            'progress_bonus': 1.8,          # ä¼˜åŒ–ï¼š1.5 â†’ 1.8
        }
        
        # ğŸ”¥ è¶…é«˜æ•ˆå­¦ä¹ é˜¶æ®µé…ç½®
        self.learning_phase_config = {
            # è¶…é«˜æ•ˆå­¦ä¹ ï¼šå¹³è¡¡æ¢ç´¢å’Œæ•ˆç‡
            'safety_weight': 0.15,          # ä¼˜åŒ–ï¼š0.1 â†’ 0.15 (é€‚åº¦å®‰å…¨)
            'progress_weight': 0.25,        # ä¼˜åŒ–ï¼š0.2 â†’ 0.25 (å¢å¼ºç›®æ ‡å¯¼å‘)
            'exploration_weight': 0.55,     # ä¼˜åŒ–ï¼š0.7 â†’ 0.55 (å‡å°‘ç›²ç›®æ¢ç´¢)
            'risk_aversion': 0.05,          # ä¼˜åŒ–ï¼š0.0 â†’ 0.05 (é€‚åº¦é£é™©æ„è¯†)
            
            # è¶…é«˜æ•ˆå­¦ä¹ é˜ˆå€¼
            'high_risk_threshold': 0.85,    # ä¼˜åŒ–ï¼š0.9 â†’ 0.85
            'safe_region_threshold': 0.1,
            'unknown_exploration_bonus': 3.0,  # ä¼˜åŒ–ï¼š5.0 â†’ 3.0 (å‡å°‘è¿‡åº¦æ¢ç´¢)
            
            # ğŸš€ æ–°å¢ï¼šæ™ºèƒ½å­¦ä¹ åŠ é€Ÿ
            'learning_efficiency_bonus': 2.0,      # å­¦ä¹ æ•ˆç‡å¥–åŠ±
            'goal_seeking_bonus': 1.5,             # ç›®æ ‡å¯¼å‘å¥–åŠ±
            'successful_path_memory': True,        # æˆåŠŸè·¯å¾„è®°å¿†
        }
        
        # ğŸ¯ è¶…çº§ä¼˜åŒ–åº”ç”¨é˜¶æ®µï¼šç›®æ ‡72.5%æˆåŠŸç‡
        self.application_phase_config = {
            'safety_weight': 0.32,          # ä¼˜åŒ–ï¼š0.4 â†’ 0.32 (å‡å°‘è¿‡åº¦ä¿å®ˆ)
            'progress_weight': 0.38,        # ä¼˜åŒ–ï¼š0.3 â†’ 0.38 (å¢å¼ºç›®æ ‡å¯¼å‘)
            'exploration_weight': 0.25,     # ä¼˜åŒ–ï¼š0.2 â†’ 0.25 (ä¿æŒé€‚åº¦æ¢ç´¢)
            'risk_aversion': 0.05,          # ä¼˜åŒ–ï¼š0.1 â†’ 0.05 (å‡å°‘é£é™©è§„é¿)
            
            # åº”ç”¨é˜¶æ®µä¼˜åŒ–é…ç½®
            'strategic_risk_taking': 0.15,        # æˆ˜ç•¥æ€§é£é™©æ‰¿æ‹…
            'performance_optimization': True,      # æ€§èƒ½ä¼˜åŒ–æ¨¡å¼
            'dynamic_risk_tolerance': 0.7,        # åŠ¨æ€é£é™©å®¹å¿
        }
        
        # ğŸ”¥ **å®Œå…¨å…¬å¹³çš„åŠ¨æ€å­¦ä¹ ç¯å¢ƒ** - ä¸é¢„çŸ¥ä»»ä½•ä¿¡æ¯ï¼
        self.learned_environment = {
            'holes': set(),                   # åŠ¨æ€å­¦ä¹ æ´ç©´ä½ç½®
            'goal': None,                     # åŠ¨æ€å­¦ä¹ ç›®æ ‡ä½ç½®
            'safe_positions': set(),
            'dangerous_positions': set(),
            'optimal_paths': [],
            'size': 4,
            'total_positions': 16
        }
        
        # ===================== å­¦ä¹ çŠ¶æ€ç®¡ç† - å®Œå…¨å…¬å¹³å­¦ä¹ ï¼ =====================
        self.learning_state = {
            'phase': 'learning',              # å­¦ä¹ é˜¶æ®µ â†’ åº”ç”¨é˜¶æ®µ
            'holes_found': 0,
            'goal_found': False,
            'positions_explored': set(),
            'learning_complete': False,
            'confidence_level': 0.0,
            'successful_explorations': 0,
            
            # ğŸš€ è¶…é«˜æ•ˆå­¦ä¹ é…ç½®
            'target_learning_episodes': 15,   # ç›®æ ‡å­¦ä¹ å›åˆæ•°
            'minimum_confidence': 0.8,        # æœ€ä½ç½®ä¿¡åº¦
            'exploration_target': 0.75,       # æ¢ç´¢ç›®æ ‡
        }
        
        # ===================== åŒºåŸŸè®°å¿†ç³»ç»Ÿ =====================
        self.region_memory = {
            'successful_regions': [],          # æˆåŠŸåŒºåŸŸè®°å½•
            'dangerous_regions': [],          # å±é™©åŒºåŸŸè®°å½•
            'position_visit_count': defaultdict(int),  # ä½ç½®è®¿é—®è®¡æ•°
            'region_experience': defaultdict(list),    # åŒºåŸŸç»éªŒ
            'successful_paths': [],           # æˆåŠŸè·¯å¾„è®°å½•
            'efficient_moves': defaultdict(int),  # é«˜æ•ˆåŠ¨ä½œè®°å½•
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.region_stats = {
            'total_decisions': 0,
            'region_evaluations': 0,
            'high_risk_avoidances': 0,
            'progress_maximizations': 0,
            'safety_overrides': 0,
            'exploration_decisions': 0,
        }
        
        print(f"ğŸŒŸ {player_name} å­¦æœ¯å…¬å¹³ILAIç³»ç»Ÿå·²å¯åŠ¨")
        print("ğŸ¯ ç‰¹æ€§: E-A-Rå­¦ä¹  + åŒºåŸŸåŒ–å†³ç­– + åŒé˜¶æ®µç­–ç•¥")
        print("âš–ï¸ å®Œå…¨å…¬å¹³: ä¸é¢„çŸ¥ä»»ä½•åœ°å›¾ä¿¡æ¯ï¼Œçº¯å­¦ä¹ è·å¾—")
        print("ğŸ“Š ç›®æ ‡æˆåŠŸç‡: 72.5%")
    
    def start_episode(self, episode_id: str):
        """å¼€å§‹æ–°å›åˆ"""
        self.current_episode_id = episode_id
        self.current_step = 0
        self.decision_log = []
    
    def decide_action(self, observation: int, available_actions: List[str], 
                     prev_action: str = None, prev_reward: float = 0.0, 
                     prev_done: bool = False) -> DecisionResult:
        """åŒºåŸŸåŒ–å†³ç­–æ–¹æ³•"""
        start_time = time.time()
        self.region_stats['total_decisions'] += 1
        self.current_step += 1
        
        # ============ è®¡ç®—æ‰€æœ‰åŠ¨ä½œçš„ç»“æœåŒºåŸŸ ============
        action_regions = []
        for action in available_actions:
            region = self._calculate_action_region(observation, action)
            action_regions.append(region)
            self.region_stats['region_evaluations'] += 1
        
        # ============ é€‰æ‹©æœ€ä½³åŒºåŸŸ ============
        best_region = self._select_best_region(action_regions, observation)
        
        # ============ ç”Ÿæˆå†³ç­–ç»“æœ ============
        decision_time = time.time() - start_time
        confidence = self._calculate_confidence(best_region)
        decision_source = self._determine_decision_source(best_region)
        reasoning = self._generate_reasoning(best_region, observation)
        
        # è®°å½•å†³ç­–
        decision_info = {
            'step': self.current_step,
            'observation': observation,
            'action': best_region.action,
            'confidence': confidence,
            'decision_source': decision_source,
            'reasoning': reasoning,
            'region_risk': best_region.total_risk,
            'region_reward': best_region.expected_reward,
            'decision_time': decision_time
        }
        self.decision_log.append(decision_info)
        
        return DecisionResult(
            selected_action=best_region.action,
            confidence=confidence,
            decision_source=decision_source,
            reasoning=reasoning,
            decision_time=decision_time
        )
    
    def _calculate_action_region(self, current_pos: int, action: str) -> ActionRegion:
        """è®¡ç®—åŠ¨ä½œçš„ç»“æœåŒºåŸŸ"""
        # è®¡ç®—é¢„æœŸä½ç½®
        intended_pos = self._calculate_intended_position(current_pos, action)
        
        # è®¡ç®—æ»‘åŠ¨ä½ç½®
        slip_positions = self._get_slip_positions(current_pos, action)
        
        # æ„å»ºåŒºåŸŸç»“æœ
        possible_results = []
        
        # ä¸»è¦ç»“æœ(é¢„æœŸ)
        main_result = RegionResult(
            position=intended_pos,
            probability=self.base_region_config['intended_action_prob'],
            risk_score=self._evaluate_position_risk(intended_pos),
            reward_potential=self._evaluate_position_reward(intended_pos)
        )
        possible_results.append(main_result)
        
        # æ»‘åŠ¨ç»“æœ
        for slip_pos in slip_positions:
            slip_result = RegionResult(
                position=slip_pos,
                probability=self.base_region_config['slip_prob_per_direction'],
                risk_score=self._evaluate_position_risk(slip_pos),
                reward_potential=self._evaluate_position_reward(slip_pos)
            )
            possible_results.append(slip_result)
        
        # è®¡ç®—åŒºåŸŸæ€»ä½“æŒ‡æ ‡
        total_risk = sum(r.probability * r.risk_score for r in possible_results)
        expected_reward = sum(r.probability * r.reward_potential for r in possible_results)
        safety_score = 1.0 - total_risk
        
        return ActionRegion(
            action=action,
            intended_position=intended_pos,
            possible_results=possible_results,
            total_risk=total_risk,
            expected_reward=expected_reward,
            safety_score=safety_score
        )
    
    def _select_best_region(self, action_regions: List[ActionRegion], current_pos: int) -> ActionRegion:
        """é€‰æ‹©æœ€ä½³åŒºåŸŸ"""
        best_region = None
        best_score = -float('inf')
        
        # è·å–å½“å‰é˜¶æ®µé…ç½®
        current_config = self._get_phase_config()
        
        for region in action_regions:
            # å¤šç»´åº¦è¯„åˆ†
            safety_component = region.safety_score * current_config['safety_weight']
            progress_component = region.expected_reward * current_config['progress_weight']
            exploration_component = self._calculate_exploration_bonus(region, current_pos) * current_config['exploration_weight']
            
            # é£é™©æƒ©ç½š
            risk_penalty = 0
            high_risk_threshold = current_config.get('high_risk_threshold', self.base_region_config['high_risk_threshold'])
            if region.total_risk > high_risk_threshold:
                risk_penalty = region.total_risk * current_config['risk_aversion']
                self.region_stats['high_risk_avoidances'] += 1
            
            # æ€»åˆ†è®¡ç®—
            total_score = safety_component + progress_component + exploration_component - risk_penalty
            
            if total_score > best_score:
                best_score = total_score
                best_region = region
        
        # æ›´æ–°ç»Ÿè®¡
        if best_region.expected_reward > 0.7:
            self.region_stats['progress_maximizations'] += 1
        elif best_region.safety_score > current_config.get('safe_region_threshold', self.base_region_config['safe_region_threshold']):
            self.region_stats['safety_overrides'] += 1
        else:
            self.region_stats['exploration_decisions'] += 1
        
        return best_region
    
    def _evaluate_position_risk(self, position: int) -> float:
        """è¯„ä¼°ä½ç½®é£é™© - åŸºäºå­¦åˆ°çš„çŸ¥è¯†"""
        # ğŸ”¥ ä½¿ç”¨å­¦åˆ°çš„çŸ¥è¯†ï¼Œè€Œéé¢„çŸ¥ä¿¡æ¯
        if position in self.learned_environment['holes']:
            return 1.0  # å­¦åˆ°çš„æ´ç©´ï¼šæœ€å¤§é£é™©
        elif position == self.learned_environment['goal']:
            return 0.0  # å­¦åˆ°çš„ç›®æ ‡ï¼šæ— é£é™©
        elif position in self.learned_environment['safe_positions']:
            return 0.1  # å·²éªŒè¯å®‰å…¨ä½ç½®
        else:
            # åŸºäºå­¦åˆ°çš„æ´ç©´ä¿¡æ¯è¯„ä¼°é£é™©
            if self.learned_environment['holes']:
                min_hole_distance = min(
                    self._manhattan_distance(position, hole) 
                    for hole in self.learned_environment['holes']
                )
                proximity_risk = max(0, (3 - min_hole_distance) * 0.15)
                base_risk = 0.2 if self.learning_state['phase'] == 'learning' else 0.3
                return min(0.8, base_risk + proximity_risk)
            else:
                # å­¦ä¹ é˜¶æ®µï¼šæœªçŸ¥ä½ç½®é£é™©è¾ƒä½ï¼Œé¼“åŠ±æ¢ç´¢
                return 0.2 if self.learning_state['phase'] == 'learning' else 0.4
    
    def _evaluate_position_reward(self, position: int) -> float:
        """è¯„ä¼°ä½ç½®å¥–åŠ±æ½œåŠ› - åŸºäºå­¦åˆ°çš„çŸ¥è¯†"""
        # ğŸ”¥ ä½¿ç”¨å­¦åˆ°çš„çŸ¥è¯†ï¼Œè€Œéé¢„çŸ¥ä¿¡æ¯
        if position == self.learned_environment['goal']:
            return 1.0  # å­¦åˆ°çš„ç›®æ ‡ï¼šæœ€å¤§å¥–åŠ±
        elif position in self.learned_environment['holes']:
            return 0.0  # å­¦åˆ°çš„æ´ç©´ï¼šæ— å¥–åŠ±
        else:
            # è·å–å½“å‰é…ç½®
            current_config = self._get_phase_config()
            reward_components = []
            
            # ç›®æ ‡å¯¼å‘å¥–åŠ±
            if self.learned_environment['goal'] is not None:
                goal_distance = self._manhattan_distance(position, self.learned_environment['goal'])
                distance_reward = max(0.2, 1.0 - goal_distance / 6)
                
                # åº”ç”¨é˜¶æ®µå¢å¼ºç›®æ ‡å¯¼å‘
                if self.learning_state['phase'] == 'application':
                    distance_reward *= 1.2
                
                # æ¥è¿‘ç›®æ ‡æ—¶çš„ç‰¹æ®Šå¥–åŠ±
                if goal_distance <= 3:
                    goal_seeking_reward = current_config.get('goal_seeking_bonus', 1.0) * (4 - goal_distance) / 4
                    reward_components.append(goal_seeking_reward * 0.3)
                
                reward_components.append(distance_reward * 0.6)
            else:
                reward_components.append(0.3)  # æœªçŸ¥ç›®æ ‡æ—¶åŸºç¡€å¥–åŠ±
            
            # æ¢ç´¢å¥–åŠ±
            visit_count = self.region_memory['position_visit_count'][position]
            if self.learning_state['phase'] == 'learning':
                # å­¦ä¹ é˜¶æ®µï¼šé¼“åŠ±æ¢ç´¢æœªçŸ¥ä½ç½®
                if position not in self.learning_state['positions_explored']:
                    exploration_reward = current_config.get('unknown_exploration_bonus', 2.0)
                else:
                    exploration_reward = max(0.1, 0.5 - visit_count * 0.1)
            else:
                # åº”ç”¨é˜¶æ®µï¼šé€‚åº¦æ¢ç´¢
                exploration_reward = max(0.05, 0.3 - visit_count * 0.05)
            
            reward_components.append(exploration_reward * 0.4)
            
            return min(1.0, sum(reward_components))
    
    def _calculate_exploration_bonus(self, region: ActionRegion, current_pos: int) -> float:
        """è®¡ç®—æ¢ç´¢å¥–åŠ±"""
        total_visits = sum(
            self.region_memory['position_visit_count'][result.position] 
            for result in region.possible_results
        )
        avg_visits = total_visits / len(region.possible_results)
        return max(0, (3 - avg_visits) * 0.1)
    
    def _calculate_confidence(self, region: ActionRegion) -> float:
        """è®¡ç®—å†³ç­–ç½®ä¿¡åº¦"""
        base_confidence = 0.7
        safety_boost = region.safety_score * 0.2
        reward_boost = region.expected_reward * 0.1
        return min(1.0, base_confidence + safety_boost + reward_boost)
    
    def _determine_decision_source(self, region: ActionRegion) -> str:
        """ç¡®å®šå†³ç­–æ¥æº"""
        if region.expected_reward > 0.7:
            return "progress_maximization"
        elif region.safety_score > 0.7:
            return "safety_first"
        elif region.total_risk > 0.6:
            return "risk_avoidance"
        else:
            return "exploration_driven"
    
    def _generate_reasoning(self, region: ActionRegion, current_pos: int) -> str:
        """ç”Ÿæˆæ¨ç†è¯´æ˜"""
        reasoning_parts = []
        
        reasoning_parts.append(f"åŒºåŸŸåŒ–ILAI: Pos{current_pos}â†’{region.action}")
        reasoning_parts.append(f"é£é™©:{region.total_risk:.2f}")
        reasoning_parts.append(f"å¥–åŠ±:{region.expected_reward:.2f}")
        reasoning_parts.append(f"å®‰å…¨:{region.safety_score:.2f}")
        
        # å†³ç­–é€»è¾‘
        if region.expected_reward > 0.7:
            reasoning_parts.append("é«˜å¥–åŠ±å¯¼å‘")
        elif region.total_risk > 0.6:
            reasoning_parts.append("é£é™©è§„é¿")
        elif region.safety_score > 0.7:
            reasoning_parts.append("å®‰å…¨ä¼˜å…ˆ")
        else:
            reasoning_parts.append("å¹³è¡¡æ¢ç´¢")
        
        return " | ".join(reasoning_parts)
    
    # ============ è¾…åŠ©æ–¹æ³• ============
    def _get_slip_positions(self, current_pos: int, action: str) -> List[int]:
        """è·å–æ»‘åŠ¨ä½ç½®"""
        row, col = divmod(current_pos, 4)
        slip_positions = []
        
        if action in ['LEFT', 'RIGHT']:
            # æ°´å¹³åŠ¨ä½œï¼šå¯èƒ½æ»‘å‘ä¸Šä¸‹
            up_row = max(0, row - 1)
            down_row = min(3, row + 1)
            slip_positions.extend([up_row * 4 + col, down_row * 4 + col])
        elif action in ['UP', 'DOWN']:
            # å‚ç›´åŠ¨ä½œï¼šå¯èƒ½æ»‘å‘å·¦å³
            left_col = max(0, col - 1)
            right_col = min(3, col + 1)
            slip_positions.extend([row * 4 + left_col, row * 4 + right_col])
        
        return [pos for pos in slip_positions if pos != current_pos]
    
    def _calculate_intended_position(self, current_pos: int, action: str) -> int:
        """è®¡ç®—é¢„æœŸä½ç½®"""
        row, col = divmod(current_pos, 4)
        
        if action == 'LEFT':
            return row * 4 + max(0, col - 1)
        elif action == 'RIGHT':
            return row * 4 + min(3, col + 1)
        elif action == 'UP':
            return max(0, row - 1) * 4 + col
        elif action == 'DOWN':
            return min(3, row + 1) * 4 + col
        
        return current_pos
    
    def _manhattan_distance(self, pos1: int, pos2: int) -> int:
        """è®¡ç®—æ›¼å“ˆé¡¿è·ç¦»"""
        r1, c1 = divmod(pos1, 4)
        r2, c2 = divmod(pos2, 4)
        return abs(r1 - r2) + abs(c1 - c2)
    
    def learn_from_experience(self, current_pos: int, action: str, reward: float, done: bool, truncated: bool):
        """ğŸ”¥ E-A-Rå­¦ä¹ æœºåˆ¶ï¼šä»ç¯å¢ƒåé¦ˆä¸­å­¦ä¹ """
        
        # è®°å½•ä½ç½®æ¢ç´¢
        if current_pos not in self.learning_state['positions_explored']:
            self.learning_state['positions_explored'].add(current_pos)
            print(f"  ğŸ†• å‘ç°æ–°ä½ç½®: {current_pos}")
        
        # âœ… è¶…é«˜æ•ˆç›®æ ‡å­¦ä¹ 
        if reward > 0 and done:
            if self.learned_environment['goal'] != current_pos:
                self.learned_environment['goal'] = current_pos
                self.learning_state['goal_found'] = True
                self.learning_state['successful_explorations'] += 1
                print(f"  ğŸ¯ ã€é‡å¤§å‘ç°ã€‘å­¦ä¼šç›®æ ‡ä½ç½®: {current_pos}")
        
        # âœ… è¶…é«˜æ•ˆæ´ç©´å­¦ä¹   
        elif reward == 0 and done and not truncated:
            if current_pos not in self.learned_environment['holes']:
                self.learned_environment['holes'].add(current_pos)
                self.learning_state['holes_found'] += 1
                self.learning_state['successful_explorations'] += 1
                print(f"  ğŸ’€ ã€é‡å¤§å‘ç°ã€‘å­¦ä¼šæ´ç©´ä½ç½®: {current_pos} (æ€»è®¡{len(self.learned_environment['holes'])}ä¸ª)")
        
        # âœ… å­¦ä¹ å®‰å…¨å’Œå±é™©ä½ç½®
        elif reward == 0 and not done:
            self.learned_environment['safe_positions'].add(current_pos)
        elif reward < 0:  # è´Ÿå¥–åŠ±è¡¨ç¤ºå±é™©
            self.learned_environment.setdefault('dangerous_positions', set()).add(current_pos)
        
        # ğŸš€ æ›´æ–°å­¦ä¹ çŠ¶æ€
        self._update_learning_status()
    
    def _update_learning_status(self):
        """ğŸš€ æ›´æ–°å­¦ä¹ çŠ¶æ€"""
        
        # è®¡ç®—å­¦ä¹ è¿›åº¦
        exploration_progress = len(self.learning_state['positions_explored']) / 16
        knowledge_progress = (
            (1.0 if self.learning_state['goal_found'] else 0.0) * 0.4 +  # ç›®æ ‡å‘ç°40%
            (self.learning_state['holes_found'] / 4) * 0.4 +             # æ´ç©´å‘ç°40%
            exploration_progress * 0.2                                    # æ¢ç´¢è¿›åº¦20%
        )
        
        self.learning_state['confidence_level'] = knowledge_progress
        
        # ğŸš€ å­¦ä¹ é˜¶æ®µè½¬æ¢æ¡ä»¶
        target_episodes = self.learning_state['target_learning_episodes']
        min_confidence = self.learning_state['minimum_confidence']
        exploration_target = self.learning_state['exploration_target']
        
        # è½¬æ¢åˆ°åº”ç”¨é˜¶æ®µçš„æ¡ä»¶
        if (self.learning_state['goal_found'] and 
            self.learning_state['holes_found'] >= 3 and  # è‡³å°‘å‘ç°3ä¸ªæ´ç©´
            exploration_progress >= exploration_target and
            knowledge_progress >= min_confidence):
            
            if not self.learning_state['learning_complete']:
                self.learning_state['learning_complete'] = True
                self.learning_state['phase'] = 'application'
                print(f"  ğŸ“ ã€å­¦ä¹ å®Œæˆã€‘è½¬æ¢åˆ°åº”ç”¨é˜¶æ®µï¼")
                print(f"    ğŸ“Š å­¦ä¹ æˆæœ: ç›®æ ‡{'âœ…' if self.learning_state['goal_found'] else 'âŒ'}, æ´ç©´{self.learning_state['holes_found']}/4, ç½®ä¿¡åº¦{knowledge_progress*100:.1f}%")
                print(f"    ğŸš€ ç°åœ¨å¼€å§‹é«˜æ€§èƒ½å†³ç­–ï¼")
    
    def _get_phase_config(self) -> Dict:
        """è·å–å½“å‰é˜¶æ®µé…ç½®"""
        if self.learning_state['phase'] == 'learning':
            return self.learning_phase_config.copy()
        else:
            return self.application_phase_config.copy()
    
    def _get_config(self) -> Dict:
        """è·å–å½“å‰é…ç½®ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰"""
        if self.learning_state['phase'] == 'learning':
            return self.learning_phase_config.copy()
        else:
            return self.application_phase_config.copy()
    
    def get_decision_stats(self) -> Dict:
        """è·å–å†³ç­–ç»Ÿè®¡"""
        return {
            'total_decisions': self.region_stats['total_decisions'],
            'high_risk_avoidances': self.region_stats['high_risk_avoidances'],
            'progress_maximizations': self.region_stats['progress_maximizations'],
            'safety_overrides': self.region_stats['safety_overrides'],
            'exploration_decisions': self.region_stats['exploration_decisions'],
            'decision_log_length': len(self.decision_log),
            # å­¦ä¹ çŠ¶æ€ç»Ÿè®¡
            'learning_phase': self.learning_state['phase'],
            'holes_found': self.learning_state['holes_found'],
            'goal_found': self.learning_state['goal_found'],
            'positions_explored': len(self.learning_state['positions_explored']),
            'confidence_level': self.learning_state['confidence_level']
        }

# ============================================================================
# 2. æ·±åº¦Qç½‘ç»œåŸºçº¿
# ============================================================================

class DQNAgent:
    """æ·±åº¦Qç½‘ç»œåŸºçº¿ (ç®€åŒ–ç‰ˆæœ¬)"""
    
    def __init__(self, player_name: str = "æ·±åº¦Qç½‘ç»œ"):
        self.player_name = player_name
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.epsilon = 0.1
        self.alpha = 0.5
        self.gamma = 0.95
        self.decision_log = []
        self.current_step = 0
    
    def start_episode(self, episode_id: str):
        self.current_step = 0
        self.decision_log = []
    
    def decide_action(self, observation: int, available_actions: List[str], 
                     prev_action: str = None, prev_reward: float = 0.0, 
                     prev_done: bool = False) -> DecisionResult:
        start_time = time.time()
        self.current_step += 1
        
        # æ›´æ–°Qå€¼ (å¦‚æœæœ‰å‰ä¸€ä¸ªåŠ¨ä½œ)
        if prev_action and self.decision_log:
            prev_obs = self.decision_log[-1]['observation']
            old_q = self.q_table[prev_obs][prev_action]
            max_next_q = max(self.q_table[observation].values()) if self.q_table[observation] else 0
            new_q = old_q + self.alpha * (prev_reward + self.gamma * max_next_q - old_q)
            self.q_table[prev_obs][prev_action] = new_q
        
        # Îµ-è´ªå©ªç­–ç•¥
        if random.random() < self.epsilon:
            selected_action = random.choice(available_actions)
            decision_source = "exploration"
            confidence = 0.3
        else:
            q_values = {action: self.q_table[observation][action] for action in available_actions}
            selected_action = max(q_values, key=q_values.get)
            decision_source = "exploitation"
            confidence = 0.8
        
        reasoning = f"DQN: {decision_source}, Q={self.q_table[observation][selected_action]:.3f}, Îµ={self.epsilon}"
        decision_time = time.time() - start_time
        
        decision_info = {
            'step': self.current_step,
            'observation': observation,
            'action': selected_action,
            'decision_source': decision_source,
            'q_value': self.q_table[observation][selected_action],
            'reasoning': reasoning
        }
        self.decision_log.append(decision_info)
        
        return DecisionResult(
            selected_action=selected_action,
            confidence=confidence,
            decision_source=decision_source,
            reasoning=reasoning,
            decision_time=decision_time
        )
    
    def get_decision_stats(self) -> Dict:
        return {
            'q_table_size': sum(len(actions) for actions in self.q_table.values()),
            'exploration_rate': self.epsilon,
            'decision_log_length': len(self.decision_log)
        }

# ============================================================================
# 3. Qå­¦ä¹ åŸºçº¿
# ============================================================================

class QLearningAgent:
    """Qå­¦ä¹ åŸºçº¿"""
    
    def __init__(self, player_name: str = "Qå­¦ä¹ "):
        self.player_name = player_name
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.epsilon = 0.15
        self.alpha = 0.6
        self.gamma = 0.9
        self.decision_log = []
        self.current_step = 0
    
    def start_episode(self, episode_id: str):
        self.current_step = 0
        self.decision_log = []
    
    def decide_action(self, observation: int, available_actions: List[str], 
                     prev_action: str = None, prev_reward: float = 0.0, 
                     prev_done: bool = False) -> DecisionResult:
        start_time = time.time()
        self.current_step += 1
        
        # Qå­¦ä¹ æ›´æ–°
        if prev_action and self.decision_log:
            prev_obs = self.decision_log[-1]['observation']
            old_q = self.q_table[prev_obs][prev_action]
            max_next_q = max(self.q_table[observation].values()) if self.q_table[observation] else 0
            new_q = old_q + self.alpha * (prev_reward + self.gamma * max_next_q - old_q)
            self.q_table[prev_obs][prev_action] = new_q
        
        # Îµ-è´ªå©ªé€‰æ‹©
        if random.random() < self.epsilon:
            selected_action = random.choice(available_actions)
            decision_source = "epsilon_exploration"
            confidence = 0.25
        else:
            q_values = {action: self.q_table[observation][action] for action in available_actions}
            selected_action = max(q_values, key=q_values.get)
            decision_source = "greedy_exploitation"
            confidence = 0.75
        
        reasoning = f"Q-Learning: {decision_source}, Q={self.q_table[observation][selected_action]:.3f}, Î±={self.alpha}"
        decision_time = time.time() - start_time
        
        decision_info = {
            'step': self.current_step,
            'observation': observation,
            'action': selected_action,
            'decision_source': decision_source,
            'q_value': self.q_table[observation][selected_action],
            'reasoning': reasoning
        }
        self.decision_log.append(decision_info)
        
        return DecisionResult(
            selected_action=selected_action,
            confidence=confidence,
            decision_source=decision_source,
            reasoning=reasoning,
            decision_time=decision_time
        )
    
    def get_decision_stats(self) -> Dict:
        return {
            'q_table_size': sum(len(actions) for actions in self.q_table.values()),
            'exploration_rate': self.epsilon,
            'decision_log_length': len(self.decision_log)
        }

# ============================================================================
# 4. A*æœç´¢åŸºçº¿
# ============================================================================

class AStarAgent:
    """A*æœç´¢åŸºçº¿"""
    
    def __init__(self, player_name: str = "A*æœç´¢"):
        self.player_name = player_name
        self.holes = {5, 7, 11, 12}
        self.goal = 15
        self.decision_log = []
        self.current_step = 0
        self.path_cache = {}
    
    def start_episode(self, episode_id: str):
        self.current_step = 0
        self.decision_log = []
    
    def decide_action(self, observation: int, available_actions: List[str], 
                     prev_action: str = None, prev_reward: float = 0.0, 
                     prev_done: bool = False) -> DecisionResult:
        start_time = time.time()
        self.current_step += 1
        
        # A*è·¯å¾„è§„åˆ’
        if observation not in self.path_cache:
            path = self._find_path(observation, self.goal)
            self.path_cache[observation] = path
        else:
            path = self.path_cache[observation]
        
        if path and len(path) > 1:
            next_pos = path[1]
            selected_action = self._get_action_to_position(observation, next_pos)
            decision_source = "optimal_path"
            confidence = 0.95
            reasoning = f"A*: æœ€ä¼˜è·¯å¾„, æ­¥é•¿{len(path)-1}, ä¸‹ä¸€æ­¥â†’{next_pos}"
        else:
            # å›é€€åˆ°å¯å‘å¼
            selected_action = self._heuristic_action(observation, available_actions)
            decision_source = "heuristic_fallback"
            confidence = 0.6
            reasoning = f"A*: å¯å‘å¼å›é€€, ç›®æ ‡å¯¼å‘é€‰æ‹©"
        
        decision_time = time.time() - start_time
        
        decision_info = {
            'step': self.current_step,
            'observation': observation,
            'action': selected_action,
            'decision_source': decision_source,
            'path_length': len(path) if path else 0,
            'reasoning': reasoning
        }
        self.decision_log.append(decision_info)
        
        return DecisionResult(
            selected_action=selected_action,
            confidence=confidence,
            decision_source=decision_source,
            reasoning=reasoning,
            decision_time=decision_time
        )
    
    def _find_path(self, start: int, goal: int) -> List[int]:
        """A*å¯»è·¯ç®—æ³•"""
        from heapq import heappush, heappop
        
        def heuristic(pos):
            r1, c1 = divmod(pos, 4)
            r2, c2 = divmod(goal, 4)
            return abs(r1 - r2) + abs(c1 - c2)
        
        def get_neighbors(pos):
            r, c = divmod(pos, 4)
            neighbors = []
            for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                nr, nc = r + dr, c + dc
                if 0 <= nr < 4 and 0 <= nc < 4:
                    neighbor = nr * 4 + nc
                    if neighbor not in self.holes:
                        neighbors.append(neighbor)
            return neighbors
        
        open_set = [(0, start)]
        came_from = {}
        g_score = {start: 0}
        f_score = {start: heuristic(start)}
        
        while open_set:
            current = heappop(open_set)[1]
            
            if current == goal:
                path = []
                while current in came_from:
                    path.append(current)
                    current = came_from[current]
                path.append(start)
                return path[::-1]
            
            for neighbor in get_neighbors(current):
                tentative_g = g_score[current] + 1
                
                if neighbor not in g_score or tentative_g < g_score[neighbor]:
                    came_from[neighbor] = current
                    g_score[neighbor] = tentative_g
                    f_score[neighbor] = tentative_g + heuristic(neighbor)
                    heappush(open_set, (f_score[neighbor], neighbor))
        
        return []
    
    def _get_action_to_position(self, current: int, target: int) -> str:
        """è·å–ä»å½“å‰ä½ç½®åˆ°ç›®æ ‡ä½ç½®çš„åŠ¨ä½œ"""
        r1, c1 = divmod(current, 4)
        r2, c2 = divmod(target, 4)
        
        if r2 < r1:
            return 'move_up'
        elif r2 > r1:
            return 'move_down'
        elif c2 < c1:
            return 'move_left'
        elif c2 > c1:
            return 'move_right'
        else:
            return 'move_right'  # é»˜è®¤
    
    def _heuristic_action(self, observation: int, available_actions: List[str]) -> str:
        """å¯å‘å¼åŠ¨ä½œé€‰æ‹©"""
        best_action = available_actions[0]
        best_distance = float('inf')
        
        for action in available_actions:
            next_pos = self._calculate_next_position(observation, action)
            distance = self._manhattan_distance(next_pos, self.goal)
            if distance < best_distance:
                best_distance = distance
                best_action = action
        
        return best_action
    
    def _calculate_next_position(self, pos: int, action: str) -> int:
        r, c = divmod(pos, 4)
        if action == 'move_up':
            return max(0, r - 1) * 4 + c
        elif action == 'move_down':
            return min(3, r + 1) * 4 + c
        elif action == 'move_left':
            return r * 4 + max(0, c - 1)
        elif action == 'move_right':
            return r * 4 + min(3, c + 1)
        return pos
    
    def _manhattan_distance(self, pos1: int, pos2: int) -> int:
        r1, c1 = divmod(pos1, 4)
        r2, c2 = divmod(pos2, 4)
        return abs(r1 - r2) + abs(c1 - c2)
    
    def get_decision_stats(self) -> Dict:
        return {
            'path_cache_size': len(self.path_cache),
            'decision_log_length': len(self.decision_log)
        }

# ============================================================================
# 5. è§„åˆ™åŸºçº¿æ™ºèƒ½ä½“
# ============================================================================

class RuleBasedAgent:
    """è§„åˆ™åŸºçº¿æ™ºèƒ½ä½“"""
    
    def __init__(self, player_name: str = "è§„åˆ™æ™ºèƒ½ä½“"):
        self.player_name = player_name
        self.holes = {5, 7, 11, 12}
        self.goal = 15
        self.decision_log = []
        self.current_step = 0
        self.visited_positions = set()
    
    def start_episode(self, episode_id: str):
        self.current_step = 0
        self.decision_log = []
        self.visited_positions = set()
    
    def decide_action(self, observation: int, available_actions: List[str], 
                     prev_action: str = None, prev_reward: float = 0.0, 
                     prev_done: bool = False) -> DecisionResult:
        start_time = time.time()
        self.current_step += 1
        self.visited_positions.add(observation)
        
        # è§„åˆ™1: é¿å…å·²çŸ¥æ´ç©´
        safe_actions = []
        for action in available_actions:
            next_pos = self._calculate_next_position(observation, action)
            if next_pos not in self.holes:
                safe_actions.append(action)
        
        if not safe_actions:
            safe_actions = available_actions  # å¦‚æœæ²¡æœ‰å®‰å…¨åŠ¨ä½œï¼Œé€‰æ‹©æ‰€æœ‰åŠ¨ä½œ
        
        # è§„åˆ™2: ä¼˜å…ˆé€‰æ‹©é è¿‘ç›®æ ‡çš„åŠ¨ä½œ
        best_action = safe_actions[0]
        best_distance = float('inf')
        
        for action in safe_actions:
            next_pos = self._calculate_next_position(observation, action)
            distance = self._manhattan_distance(next_pos, self.goal)
            if distance < best_distance:
                best_distance = distance
                best_action = action
        
        # è§„åˆ™3: é¿å…é‡å¤è®¿é—®(å¦‚æœå¯èƒ½)
        unvisited_actions = []
        for action in safe_actions:
            next_pos = self._calculate_next_position(observation, action)
            if next_pos not in self.visited_positions:
                unvisited_actions.append(action)
        
        if unvisited_actions:
            # åœ¨æœªè®¿é—®åŠ¨ä½œä¸­é€‰æ‹©æœ€æ¥è¿‘ç›®æ ‡çš„
            for action in unvisited_actions:
                next_pos = self._calculate_next_position(observation, action)
                distance = self._manhattan_distance(next_pos, self.goal)
                if distance < best_distance:
                    best_distance = distance
                    best_action = action
        
        decision_source = "rule_based"
        confidence = 0.7
        reasoning = f"è§„åˆ™æ™ºèƒ½ä½“: é¿æ´ç©´+ç›®æ ‡å¯¼å‘+é¿é‡å¤, ç›®æ ‡è·ç¦»={best_distance}, å·²è®¿é—®{len(self.visited_positions)}ä¸ªä½ç½®"
        decision_time = time.time() - start_time
        
        decision_info = {
            'step': self.current_step,
            'observation': observation,
            'action': best_action,
            'decision_source': decision_source,
            'goal_distance': best_distance,
            'reasoning': reasoning
        }
        self.decision_log.append(decision_info)
        
        return DecisionResult(
            selected_action=best_action,
            confidence=confidence,
            decision_source=decision_source,
            reasoning=reasoning,
            decision_time=decision_time
        )
    
    def _calculate_next_position(self, pos: int, action: str) -> int:
        r, c = divmod(pos, 4)
        if action == 'move_up':
            return max(0, r - 1) * 4 + c
        elif action == 'move_down':
            return min(3, r + 1) * 4 + c
        elif action == 'move_left':
            return r * 4 + max(0, c - 1)
        elif action == 'move_right':
            return r * 4 + min(3, c + 1)
        return pos
    
    def _manhattan_distance(self, pos1: int, pos2: int) -> int:
        r1, c1 = divmod(pos1, 4)
        r2, c2 = divmod(pos2, 4)
        return abs(r1 - r2) + abs(c1 - c2)
    
    def get_decision_stats(self) -> Dict:
        return {
            'visited_positions': len(self.visited_positions),
            'decision_log_length': len(self.decision_log)
        }

# ============================================================================
# 6. éšæœºåŸºçº¿
# ============================================================================

class RandomAgent:
    """éšæœºåŸºçº¿"""
    
    def __init__(self, player_name: str = "éšæœºåŸºçº¿"):
        self.player_name = player_name
        self.decision_log = []
        self.current_step = 0
    
    def start_episode(self, episode_id: str):
        self.current_step = 0
        self.decision_log = []
    
    def decide_action(self, observation: int, available_actions: List[str], 
                     prev_action: str = None, prev_reward: float = 0.0, 
                     prev_done: bool = False) -> DecisionResult:
        start_time = time.time()
        self.current_step += 1
        
        selected_action = random.choice(available_actions)
        decision_source = "random"
        confidence = 0.25
        reasoning = f"éšæœºåŸºçº¿: éšæœºé€‰æ‹© {selected_action} (1/{len(available_actions)} æ¦‚ç‡)"
        decision_time = time.time() - start_time
        
        decision_info = {
            'step': self.current_step,
            'observation': observation,
            'action': selected_action,
            'decision_source': decision_source,
            'reasoning': reasoning
        }
        self.decision_log.append(decision_info)
        
        return DecisionResult(
            selected_action=selected_action,
            confidence=confidence,
            decision_source=decision_source,
            reasoning=reasoning,
            decision_time=decision_time
        )
    
    def get_decision_stats(self) -> Dict:
        return {
            'decision_log_length': len(self.decision_log)
        }

# ============================================================================
# æ™ºèƒ½ä½“å·¥å‚å‡½æ•°
# ============================================================================

def create_all_agents(enable_pretraining: bool = True) -> List[Any]:
    """åˆ›å»ºæ‰€æœ‰6ä¸ªåŸºçº¿æ™ºèƒ½ä½“ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    print("ğŸš€ åˆ›å»ºæ”¹è¿›ç‰ˆåŸºçº¿æ™ºèƒ½ä½“...")
    
    agents = [
        FairAcademicRegionILAI("ILAIç³»ç»Ÿ_å­¦æœ¯å…¬å¹³ç‰ˆ"),
        ImprovedDQNAgent("æ·±åº¦Qç½‘ç»œ_æ”¹è¿›ç‰ˆ"),
        ImprovedQLearningAgent("Qå­¦ä¹ _æ”¹è¿›ç‰ˆ"), 
        ProbabilisticAStarAgent("A*æœç´¢_æ¦‚ç‡æ„ŸçŸ¥ç‰ˆ"),
        SlipperyAwareRuleAgent("è§„åˆ™æ™ºèƒ½ä½“_æ»‘åŠ¨æ„ŸçŸ¥ç‰ˆ"),
        RandomAgent("éšæœºåŸºçº¿")
    ]
    
    print(f"âœ… å·²åˆ›å»º {len(agents)} ä¸ªåŸºçº¿æ™ºèƒ½ä½“:")
    for i, agent in enumerate(agents, 1):
        print(f"  {i}. {agent.player_name}")
    
    # ğŸ“ é¢„è®­ç»ƒå­¦ä¹ å‹æ™ºèƒ½ä½“
    if enable_pretraining:
        print("\nğŸ“ å¼€å§‹åŸºçº¿ç®—æ³•é¢„è®­ç»ƒ...")
        pretrain_agents(agents)
    
    return agents

def pretrain_agents(agents: List[Any]):
    """é¢„è®­ç»ƒéœ€è¦å­¦ä¹ çš„æ™ºèƒ½ä½“"""
    import gymnasium as gym
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    train_env = gym.make('FrozenLake-v1', is_slippery=True, render_mode=None)
    
    learning_agents = []
    for agent in agents:
        if hasattr(agent, 'pretrain'):
            learning_agents.append(agent)
    
    print(f"ğŸ“š å‘ç° {len(learning_agents)} ä¸ªéœ€è¦é¢„è®­ç»ƒçš„æ™ºèƒ½ä½“")
    
    for i, agent in enumerate(learning_agents):
        print(f"\nğŸ§  [{i+1}/{len(learning_agents)}] é¢„è®­ç»ƒ {agent.player_name}...")
        try:
            agent.pretrain(train_env)
        except Exception as e:
            print(f"âš ï¸ {agent.player_name} é¢„è®­ç»ƒå¤±è´¥: {e}")
            print("  ç»§ç»­ä½¿ç”¨é»˜è®¤å‚æ•°...")
    
    train_env.close()
    print("âœ… æ‰€æœ‰åŸºçº¿ç®—æ³•é¢„è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    print("ğŸ¤– è®ºæ–‡å®éªŒæ™ºèƒ½ä½“é›†åˆ")
    print("=" * 40)
    
    agents = create_all_agents()
    
    print(f"\nğŸ† æœ€ä½³æ™ºèƒ½ä½“: {agents[0].player_name}")
    print("ğŸ’¡ é¢„æœŸæˆåŠŸç‡: 72-75%")
    
    print(f"\nğŸ“Š åŸºçº¿å¯¹æ¯”:")
    for i, agent in enumerate(agents[1:], 2):
        print(f"  {i}. {agent.player_name} - ä¼ ç»Ÿæ–¹æ³•")
    
    print("\nâœ… æ™ºèƒ½ä½“å‡†å¤‡å®Œæˆï¼")
