#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”¨æˆ·ç®—åŠ›å¼€é”€å’Œååº”æ—¶é—´ç»Ÿè®¡ç³»ç»Ÿ
User Performance Tracker

ä¸“é—¨ç”¨äºç»Ÿè®¡å’Œåˆ†æç”¨æˆ·ï¼ˆAIæ™ºèƒ½ä½“ï¼‰çš„è®¡ç®—èµ„æºä½¿ç”¨æƒ…å†µå’Œååº”æ—¶é—´
åŒ…æ‹¬è¯¦ç»†çš„å†³ç­–è€—æ—¶ã€ç®—åŠ›æ¶ˆè€—ã€å†…å­˜ä½¿ç”¨ç­‰æ€§èƒ½æŒ‡æ ‡
"""

import time
import psutil
import threading
import os
import json
import numpy as np
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: float = field(default_factory=time.time)
    operation_type: str = ""  # æ“ä½œç±»å‹ï¼šdecision, learning, planningç­‰
    execution_time: float = 0.0  # æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    cpu_usage: float = 0.0  # CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰
    memory_usage: float = 0.0  # å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰
    memory_delta: float = 0.0  # å†…å­˜å˜åŒ–é‡ï¼ˆMBï¼‰
    thread_count: int = 0  # çº¿ç¨‹æ•°é‡
    operation_complexity: int = 1  # æ“ä½œå¤æ‚åº¦ï¼ˆ1-10ï¼‰
    success: bool = True  # æ˜¯å¦æˆåŠŸ
    error_message: str = ""  # é”™è¯¯ä¿¡æ¯
    context: Dict[str, Any] = field(default_factory=dict)  # ä¸Šä¸‹æ–‡ä¿¡æ¯

@dataclass 
class UserPerformanceProfile:
    """ç”¨æˆ·æ€§èƒ½æ¡£æ¡ˆ"""
    user_id: str = ""
    total_operations: int = 0
    total_execution_time: float = 0.0
    average_reaction_time: float = 0.0
    peak_memory_usage: float = 0.0
    average_cpu_usage: float = 0.0
    operation_success_rate: float = 1.0
    computational_efficiency: float = 1.0  # è®¡ç®—æ•ˆç‡è¯„åˆ†
    performance_trend: str = "stable"  # stable, improving, declining
    last_updated: float = field(default_factory=time.time)

class UserPerformanceTracker:
    """ç”¨æˆ·æ€§èƒ½è¿½è¸ªå™¨"""
    
    def __init__(self, user_id: str, max_history: int = 1000):
        self.user_id = user_id
        self.max_history = max_history
        
        # æ€§èƒ½æŒ‡æ ‡å†å²
        self.metrics_history: deque = deque(maxlen=max_history)
        self.operation_metrics: Dict[str, List[PerformanceMetric]] = defaultdict(list)
        
        # å®æ—¶ç›‘æ§
        self.current_operation_start = None
        self.current_operation_type = None
        self.current_memory_start = 0.0
        
        # ç»Ÿè®¡æ•°æ®
        self.profile = UserPerformanceProfile(user_id=user_id)
        self.session_stats = {
            'session_start': time.time(),
            'decisions_made': 0,
            'learning_cycles': 0,
            'planning_cycles': 0,
            'total_compute_time': 0.0,
            'peak_concurrent_operations': 0,
            'error_count': 0
        }
        
        # æ€§èƒ½é˜ˆå€¼å’Œè­¦å‘Š
        self.performance_thresholds = {
            'max_decision_time': 5.0,  # æœ€å¤§å†³ç­–æ—¶é—´ï¼ˆç§’ï¼‰
            'max_memory_usage': 1000.0,  # æœ€å¤§å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
            'max_cpu_usage': 90.0,  # æœ€å¤§CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰
            'min_success_rate': 0.8,  # æœ€å°æˆåŠŸç‡
        }
        
        # çº¿ç¨‹å®‰å…¨é”
        self.lock = threading.Lock()
        
        print(f"ğŸ“Š User performance tracker started - User ID: {user_id}")
    
    def start_operation(self, operation_type: str, complexity: int = 1, context: Dict = None):
        """å¼€å§‹ç›‘æ§ä¸€ä¸ªæ“ä½œ"""
        with self.lock:
            if self.current_operation_start is not None:
                # æœ‰æœªå®Œæˆçš„æ“ä½œï¼Œå…ˆç»“æŸå®ƒ
                self.end_operation(success=False, error="æ“ä½œè¢«ä¸­æ–­")
            
            self.current_operation_start = time.time()
            self.current_operation_type = operation_type
            self.current_memory_start = self._get_memory_usage()
            
            # è®°å½•æ“ä½œä¸Šä¸‹æ–‡
            if context:
                self.current_context = context.copy()
            else:
                self.current_context = {}
            
            self.current_complexity = complexity
    
    def end_operation(self, success: bool = True, error: str = "", result_data: Dict = None):
        """ç»“æŸç›‘æ§å½“å‰æ“ä½œ"""
        if self.current_operation_start is None:
            return None
        
        with self.lock:
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = time.time() - self.current_operation_start
            
            # è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
            memory_usage = self._get_memory_usage()
            memory_delta = memory_usage - self.current_memory_start
            cpu_usage = self._get_cpu_usage()
            thread_count = threading.active_count()
            
            # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡
            metric = PerformanceMetric(
                operation_type=self.current_operation_type,
                execution_time=execution_time,
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                memory_delta=memory_delta,
                thread_count=thread_count,
                operation_complexity=self.current_complexity,
                success=success,
                error_message=error,
                context=self.current_context.copy()
            )
            
            # æ·»åŠ ç»“æœæ•°æ®
            if result_data:
                metric.context.update(result_data)
            
            # è®°å½•æŒ‡æ ‡
            self.metrics_history.append(metric)
            self.operation_metrics[self.current_operation_type].append(metric)
            
            # æ›´æ–°ç»Ÿè®¡æ•°æ®
            self._update_statistics(metric)
            
            # æ£€æŸ¥æ€§èƒ½è­¦å‘Š
            self._check_performance_warnings(metric)
            
            # é‡ç½®å½“å‰æ“ä½œ
            self.current_operation_start = None
            self.current_operation_type = None
            self.current_memory_start = 0.0
            self.current_context = {}
            
            return metric
    
    def record_decision_time(self, decision_type: str, execution_time: float, context: Dict = None):
        """å¿«é€Ÿè®°å½•å†³ç­–æ—¶é—´"""
        with self.lock:
            metric = PerformanceMetric(
                operation_type=f"decision_{decision_type}",
                execution_time=execution_time,
                cpu_usage=self._get_cpu_usage(),
                memory_usage=self._get_memory_usage(),
                context=context or {}
            )
            
            self.metrics_history.append(metric)
            self.operation_metrics[metric.operation_type].append(metric)
            self.session_stats['decisions_made'] += 1
            
            return metric
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ€»ç»“"""
        with self.lock:
            if not self.metrics_history:
                return {"error": "æ²¡æœ‰æ€§èƒ½æ•°æ®"}
            
            # åŸºç¡€ç»Ÿè®¡
            total_metrics = len(self.metrics_history)
            total_time = sum(m.execution_time for m in self.metrics_history)
            avg_execution_time = total_time / total_metrics
            
            # æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡
            operation_stats = {}
            for op_type, metrics in self.operation_metrics.items():
                if metrics:
                    operation_stats[op_type] = {
                        'count': len(metrics),
                        'total_time': sum(m.execution_time for m in metrics),
                        'avg_time': sum(m.execution_time for m in metrics) / len(metrics),
                        'max_time': max(m.execution_time for m in metrics),
                        'min_time': min(m.execution_time for m in metrics),
                        'success_rate': sum(1 for m in metrics if m.success) / len(metrics),
                        'avg_complexity': sum(m.operation_complexity for m in metrics) / len(metrics)
                    }
            
            # èµ„æºä½¿ç”¨ç»Ÿè®¡
            memory_values = [m.memory_usage for m in self.metrics_history if m.memory_usage > 0]
            cpu_values = [m.cpu_usage for m in self.metrics_history if m.cpu_usage > 0]
            
            resource_stats = {
                'peak_memory_mb': max(memory_values) if memory_values else 0,
                'avg_memory_mb': sum(memory_values) / len(memory_values) if memory_values else 0,
                'peak_cpu_percent': max(cpu_values) if cpu_values else 0,
                'avg_cpu_percent': sum(cpu_values) / len(cpu_values) if cpu_values else 0,
                'max_threads': max(m.thread_count for m in self.metrics_history),
            }
            
            # æ€§èƒ½è¶‹åŠ¿åˆ†æ
            recent_metrics = list(self.metrics_history)[-50:]  # æœ€è¿‘50ä¸ªæ“ä½œ
            if len(recent_metrics) >= 10:
                recent_avg = sum(m.execution_time for m in recent_metrics) / len(recent_metrics)
                overall_avg = avg_execution_time
                trend = "improving" if recent_avg < overall_avg * 0.9 else "declining" if recent_avg > overall_avg * 1.1 else "stable"
            else:
                trend = "insufficient_data"
            
            # æ•ˆç‡è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰
            efficiency_score = self._calculate_efficiency_score()
            
            return {
                'user_id': self.user_id,
                'session_duration': time.time() - self.session_stats['session_start'],
                'total_operations': total_metrics,
                'total_execution_time': total_time,
                'average_execution_time': avg_execution_time,
                'performance_trend': trend,
                'efficiency_score': efficiency_score,
                'operation_statistics': operation_stats,
                'resource_statistics': resource_stats,
                'session_statistics': self.session_stats.copy(),
                'performance_warnings': self._get_performance_warnings()
            }
    
    def generate_performance_report(self, save_to_file: bool = True) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        summary = self.get_performance_summary()
        
        if "error" in summary:
            return f"æ— æ³•ç”ŸæˆæŠ¥å‘Š: {summary['error']}"
        
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append(f"ğŸ¯ ç”¨æˆ·æ€§èƒ½åˆ†ææŠ¥å‘Š - {self.user_id}")
        report_lines.append("=" * 60)
        
        # åŸºç¡€ä¿¡æ¯
        report_lines.append(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        report_lines.append(f"   ä¼šè¯æ—¶é•¿: {summary['session_duration']:.2f} ç§’")
        report_lines.append(f"   æ€»æ“ä½œæ•°: {summary['total_operations']}")
        report_lines.append(f"   æ€»è®¡ç®—æ—¶é—´: {summary['total_execution_time']:.3f} ç§’")
        report_lines.append(f"   å¹³å‡å“åº”æ—¶é—´: {summary['average_execution_time']:.3f} ç§’")
        report_lines.append(f"   æ•ˆç‡è¯„åˆ†: {summary['efficiency_score']:.1f}/10")
        report_lines.append(f"   æ€§èƒ½è¶‹åŠ¿: {summary['performance_trend']}")
        
        # æ“ä½œç±»å‹ç»Ÿè®¡
        report_lines.append(f"\nâš¡ æ“ä½œç±»å‹åˆ†æ:")
        for op_type, stats in summary['operation_statistics'].items():
            report_lines.append(f"   {op_type}:")
            report_lines.append(f"      æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
            report_lines.append(f"      å¹³å‡è€—æ—¶: {stats['avg_time']:.3f}ç§’")
            report_lines.append(f"      æœ€å¤§è€—æ—¶: {stats['max_time']:.3f}ç§’")
            report_lines.append(f"      æˆåŠŸç‡: {stats['success_rate']:.1%}")
            report_lines.append(f"      å¹³å‡å¤æ‚åº¦: {stats['avg_complexity']:.1f}")
        
        # èµ„æºä½¿ç”¨åˆ†æ
        res_stats = summary['resource_statistics']
        report_lines.append(f"\nğŸ’» èµ„æºä½¿ç”¨åˆ†æ:")
        report_lines.append(f"   å³°å€¼å†…å­˜: {res_stats['peak_memory_mb']:.1f} MB")
        report_lines.append(f"   å¹³å‡å†…å­˜: {res_stats['avg_memory_mb']:.1f} MB")
        report_lines.append(f"   å³°å€¼CPU: {res_stats['peak_cpu_percent']:.1f}%")
        report_lines.append(f"   å¹³å‡CPU: {res_stats['avg_cpu_percent']:.1f}%")
        report_lines.append(f"   æœ€å¤§çº¿ç¨‹æ•°: {res_stats['max_threads']}")
        
        # æ€§èƒ½è­¦å‘Š
        warnings = summary['performance_warnings']
        if warnings:
            report_lines.append(f"\nâš ï¸ æ€§èƒ½è­¦å‘Š:")
            for warning in warnings:
                report_lines.append(f"   â€¢ {warning}")
        else:
            report_lines.append(f"\nâœ… æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— è­¦å‘Š")
        
        # ä¼˜åŒ–å»ºè®®
        suggestions = self._generate_optimization_suggestions(summary)
        if suggestions:
            report_lines.append(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for suggestion in suggestions:
                report_lines.append(f"   â€¢ {suggestion}")
        
        report_lines.append("=" * 60)
        
        report_text = "\n".join(report_lines)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if save_to_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_report_{self.user_id}_{timestamp}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"ğŸ“„ Performance report saved: {filename}")
        
        return report_text
    
    def create_performance_charts(self, save_charts: bool = True) -> Dict[str, str]:
        """åˆ›å»ºæ€§èƒ½å›¾è¡¨"""
        if not self.metrics_history:
            return {"error": "æ²¡æœ‰æ•°æ®å¯ç”¨äºç”Ÿæˆå›¾è¡¨"}
        
        chart_files = {}
        
        try:
            # 1. å“åº”æ—¶é—´è¶‹åŠ¿å›¾
            plt.figure(figsize=(12, 6))
            
            times = [m.timestamp for m in self.metrics_history]
            exec_times = [m.execution_time for m in self.metrics_history]
            
            plt.subplot(2, 2, 1)
            plt.plot(times, exec_times, 'b-', alpha=0.7)
            plt.title('å“åº”æ—¶é—´è¶‹åŠ¿')
            plt.xlabel('æ—¶é—´')
            plt.ylabel('æ‰§è¡Œæ—¶é—´(ç§’)')
            plt.grid(True, alpha=0.3)
            
            # 2. å†…å­˜ä½¿ç”¨è¶‹åŠ¿å›¾
            memory_values = [m.memory_usage for m in self.metrics_history if m.memory_usage > 0]
            memory_times = [m.timestamp for m in self.metrics_history if m.memory_usage > 0]
            
            plt.subplot(2, 2, 2)
            if memory_values:
                plt.plot(memory_times, memory_values, 'r-', alpha=0.7)
            plt.title('å†…å­˜ä½¿ç”¨è¶‹åŠ¿')
            plt.xlabel('æ—¶é—´')
            plt.ylabel('å†…å­˜ä½¿ç”¨(MB)')
            plt.grid(True, alpha=0.3)
            
            # 3. æ“ä½œç±»å‹åˆ†å¸ƒé¥¼å›¾
            plt.subplot(2, 2, 3)
            op_counts = {op: len(metrics) for op, metrics in self.operation_metrics.items()}
            if op_counts:
                plt.pie(op_counts.values(), labels=op_counts.keys(), autopct='%1.1f%%')
            plt.title('æ“ä½œç±»å‹åˆ†å¸ƒ')
            
            # 4. CPUä½¿ç”¨ç‡åˆ†å¸ƒç›´æ–¹å›¾
            plt.subplot(2, 2, 4)
            cpu_values = [m.cpu_usage for m in self.metrics_history if m.cpu_usage > 0]
            if cpu_values:
                plt.hist(cpu_values, bins=20, alpha=0.7, color='green')
            plt.title('CPUä½¿ç”¨ç‡åˆ†å¸ƒ')
            plt.xlabel('CPUä½¿ç”¨ç‡(%)')
            plt.ylabel('é¢‘æ¬¡')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            if save_charts:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                chart_filename = f"performance_charts_{self.user_id}_{timestamp}.png"
                plt.savefig(chart_filename, dpi=300, bbox_inches='tight')
                chart_files['overview'] = chart_filename
                print(f"ğŸ“ˆ Performance chart saved: {chart_filename}")
            
            plt.close()
            
        except Exception as e:
            print(f"Error generating chart: {str(e)}")
            chart_files['error'] = str(e)
        
        return chart_files
    
    def export_raw_data(self, format: str = 'json') -> str:
        """å¯¼å‡ºåŸå§‹æ€§èƒ½æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format.lower() == 'json':
            filename = f"performance_data_{self.user_id}_{timestamp}.json"
            
            # å‡†å¤‡æ•°æ®
            export_data = {
                'user_id': self.user_id,
                'export_time': timestamp,
                'session_stats': self.session_stats,
                'performance_profile': {
                    'user_id': self.profile.user_id,
                    'total_operations': self.profile.total_operations,
                    'total_execution_time': self.profile.total_execution_time,
                    'average_reaction_time': self.profile.average_reaction_time,
                    'peak_memory_usage': self.profile.peak_memory_usage,
                    'performance_trend': self.profile.performance_trend
                },
                'metrics_history': []
            }
            
            # è½¬æ¢æŒ‡æ ‡å†å²
            for metric in self.metrics_history:
                export_data['metrics_history'].append({
                    'timestamp': metric.timestamp,
                    'operation_type': metric.operation_type,
                    'execution_time': metric.execution_time,
                    'cpu_usage': metric.cpu_usage,
                    'memory_usage': metric.memory_usage,
                    'memory_delta': metric.memory_delta,
                    'thread_count': metric.thread_count,
                    'operation_complexity': metric.operation_complexity,
                    'success': metric.success,
                    'error_message': metric.error_message,
                    'context': metric.context
                })
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            
        elif format.lower() == 'csv':
            import csv
            filename = f"performance_data_{self.user_id}_{timestamp}.csv"
            
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # å†™å…¥æ ‡é¢˜è¡Œ
                headers = ['timestamp', 'operation_type', 'execution_time', 'cpu_usage', 
                          'memory_usage', 'memory_delta', 'thread_count', 'operation_complexity',
                          'success', 'error_message']
                writer.writerow(headers)
                
                # å†™å…¥æ•°æ®è¡Œ
                for metric in self.metrics_history:
                    row = [
                        metric.timestamp, metric.operation_type, metric.execution_time,
                        metric.cpu_usage, metric.memory_usage, metric.memory_delta,
                        metric.thread_count, metric.operation_complexity,
                        metric.success, metric.error_message
                    ]
                    writer.writerow(row)
        
        print(f"ğŸ“ Raw data exported: {filename}")
        return filename
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            process = psutil.Process(os.getpid())
            return 50.0  # ä¿®å¤: å›ºå®šå€¼é¿å…ç³»ç»Ÿè°ƒç”¨
        except:
            return 0.0
    
    def _get_cpu_usage(self) -> float:
        """è·å–å½“å‰CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰"""
        try:
            process = psutil.Process(os.getpid())
            return 0.0  # ä¿®å¤: é¿å…é˜»å¡è°ƒç”¨
        except:
            return 0.0
    
    def _update_statistics(self, metric: PerformanceMetric):
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        self.profile.total_operations += 1
        self.profile.total_execution_time += metric.execution_time
        self.profile.average_reaction_time = (
            self.profile.total_execution_time / self.profile.total_operations
        )
        
        if metric.memory_usage > self.profile.peak_memory_usage:
            self.profile.peak_memory_usage = metric.memory_usage
        
        self.profile.last_updated = time.time()
        
        # æ›´æ–°ä¼šè¯ç»Ÿè®¡
        self.session_stats['total_compute_time'] += metric.execution_time
        if not metric.success:
            self.session_stats['error_count'] += 1
        
        # æ›´æ–°æ“ä½œè®¡æ•°
        if 'decision' in metric.operation_type:
            self.session_stats['decisions_made'] += 1
        elif 'learning' in metric.operation_type:
            self.session_stats['learning_cycles'] += 1
        elif 'planning' in metric.operation_type:
            self.session_stats['planning_cycles'] += 1
    
    def _check_performance_warnings(self, metric: PerformanceMetric):
        """æ£€æŸ¥æ€§èƒ½è­¦å‘Š"""
        warnings = []
        
        if metric.execution_time > self.performance_thresholds['max_decision_time']:
            warnings.append(f"æ“ä½œè€—æ—¶è¿‡é•¿: {metric.execution_time:.2f}ç§’ (é˜ˆå€¼: {self.performance_thresholds['max_decision_time']}ç§’)")
        
        if metric.memory_usage > self.performance_thresholds['max_memory_usage']:
            warnings.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {metric.memory_usage:.1f}MB (é˜ˆå€¼: {self.performance_thresholds['max_memory_usage']}MB)")
        
        if metric.cpu_usage > self.performance_thresholds['max_cpu_usage']:
            warnings.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {metric.cpu_usage:.1f}% (é˜ˆå€¼: {self.performance_thresholds['max_cpu_usage']}%)")
        
        for warning in warnings:
            print(f"âš ï¸ Performance warning [{self.user_id}]: {warning}")
    
    def _get_performance_warnings(self) -> List[str]:
        """è·å–å½“å‰çš„æ€§èƒ½è­¦å‘Š"""
        warnings = []
        
        if not self.metrics_history:
            return warnings
        
        # æ£€æŸ¥æœ€è¿‘çš„æ€§èƒ½æŒ‡æ ‡
        recent_metrics = list(self.metrics_history)[-10:]
        
        # å¹³å‡å“åº”æ—¶é—´æ£€æŸ¥
        avg_response_time = sum(m.execution_time for m in recent_metrics) / len(recent_metrics)
        if avg_response_time > self.performance_thresholds['max_decision_time']:
            warnings.append(f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.2f}ç§’")
        
        # å†…å­˜ä½¿ç”¨æ£€æŸ¥
        max_memory = max(m.memory_usage for m in recent_metrics)
        if max_memory > self.performance_thresholds['max_memory_usage']:
            warnings.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {max_memory:.1f}MB")
        
        # æˆåŠŸç‡æ£€æŸ¥
        success_rate = sum(1 for m in recent_metrics if m.success) / len(recent_metrics)
        if success_rate < self.performance_thresholds['min_success_rate']:
            warnings.append(f"æ“ä½œæˆåŠŸç‡è¿‡ä½: {success_rate:.1%}")
        
        return warnings
    
    def _calculate_efficiency_score(self) -> float:
        """è®¡ç®—æ•ˆç‡è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰"""
        if not self.metrics_history:
            return 5.0
        
        # åŸºç¡€å¾—åˆ†
        base_score = 5.0
        
        # å“åº”æ—¶é—´è¯„åˆ†ï¼ˆ40%æƒé‡ï¼‰
        avg_time = sum(m.execution_time for m in self.metrics_history) / len(self.metrics_history)
        if avg_time < 0.1:
            time_score = 10.0
        elif avg_time < 0.5:
            time_score = 8.0
        elif avg_time < 1.0:
            time_score = 6.0
        elif avg_time < 2.0:
            time_score = 4.0
        else:
            time_score = 2.0
        
        # æˆåŠŸç‡è¯„åˆ†ï¼ˆ30%æƒé‡ï¼‰
        success_rate = sum(1 for m in self.metrics_history if m.success) / len(self.metrics_history)
        success_score = success_rate * 10
        
        # èµ„æºä½¿ç”¨è¯„åˆ†ï¼ˆ30%æƒé‡ï¼‰
        memory_values = [m.memory_usage for m in self.metrics_history if m.memory_usage > 0]
        if memory_values:
            avg_memory = sum(memory_values) / len(memory_values)
            if avg_memory < 100:
                resource_score = 10.0
            elif avg_memory < 200:
                resource_score = 8.0
            elif avg_memory < 500:
                resource_score = 6.0
            elif avg_memory < 1000:
                resource_score = 4.0
            else:
                resource_score = 2.0
        else:
            resource_score = 5.0
        
        # åŠ æƒå¹³å‡
        efficiency_score = (time_score * 0.4 + success_score * 0.3 + resource_score * 0.3)
        
        return min(10.0, max(1.0, efficiency_score))
    
    def _generate_optimization_suggestions(self, summary: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # å“åº”æ—¶é—´ä¼˜åŒ–
        if summary['average_execution_time'] > 2.0:
            suggestions.append("è€ƒè™‘ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦ä»¥å‡å°‘å“åº”æ—¶é—´")
        
        # å†…å­˜ä¼˜åŒ–
        res_stats = summary['resource_statistics']
        if res_stats['peak_memory_mb'] > 500:
            suggestions.append("è€ƒè™‘å®æ–½å†…å­˜ç®¡ç†ç­–ç•¥ï¼Œå¦‚ç¼“å­˜æ¸…ç†")
        
        # æ“ä½œä¼˜åŒ–
        for op_type, stats in summary['operation_statistics'].items():
            if stats['avg_time'] > 3.0:
                suggestions.append(f"ä¼˜åŒ–{op_type}æ“ä½œçš„æ‰§è¡Œæ•ˆç‡")
            if stats['success_rate'] < 0.9:
                suggestions.append(f"æé«˜{op_type}æ“ä½œçš„æˆåŠŸç‡")
        
        # è¶‹åŠ¿ä¼˜åŒ–
        if summary['performance_trend'] == 'declining':
            suggestions.append("æ€§èƒ½å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®è¿›è¡Œå…¨é¢æ€§èƒ½å®¡æŸ¥")
        
        # æ•ˆç‡ä¼˜åŒ–
        if summary['efficiency_score'] < 6.0:
            suggestions.append("æ•´ä½“æ•ˆç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æ ¸å¿ƒç®—æ³•å’Œæ•°æ®ç»“æ„")
        
        return suggestions

# å…¨å±€æ€§èƒ½è¿½è¸ªå™¨ç®¡ç†
_performance_trackers: Dict[str, UserPerformanceTracker] = {}

def get_performance_tracker(user_id: str) -> UserPerformanceTracker:
    """è·å–æˆ–åˆ›å»ºæ€§èƒ½è¿½è¸ªå™¨"""
    if user_id not in _performance_trackers:
        _performance_trackers[user_id] = UserPerformanceTracker(user_id)
    return _performance_trackers[user_id]

def start_operation_tracking(user_id: str, operation_type: str, complexity: int = 1, context: Dict = None):
    """å¼€å§‹æ“ä½œè¿½è¸ª"""
    tracker = get_performance_tracker(user_id)
    tracker.start_operation(operation_type, complexity, context)

def end_operation_tracking(user_id: str, success: bool = True, error: str = "", result_data: Dict = None):
    """ç»“æŸæ“ä½œè¿½è¸ª"""
    tracker = get_performance_tracker(user_id)
    return tracker.end_operation(success, error, result_data)

def record_decision_performance(user_id: str, decision_type: str, execution_time: float, context: Dict = None):
    """å¿«é€Ÿè®°å½•å†³ç­–æ€§èƒ½"""
    tracker = get_performance_tracker(user_id)
    return tracker.record_decision_time(decision_type, execution_time, context)

def generate_all_reports():
    """ä¸ºæ‰€æœ‰ç”¨æˆ·ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    reports = {}
    for user_id, tracker in _performance_trackers.items():
        reports[user_id] = tracker.generate_performance_report()
    return reports

def export_all_data(format: str = 'json'):
    """å¯¼å‡ºæ‰€æœ‰ç”¨æˆ·çš„åŸå§‹æ•°æ®"""
    exported_files = {}
    for user_id, tracker in _performance_trackers.items():
        exported_files[user_id] = tracker.export_raw_data(format)
    return exported_files

# è£…é¥°å™¨ï¼šè‡ªåŠ¨æ€§èƒ½è¿½è¸ª
def track_performance(operation_type: str, complexity: int = 1, user_id: str = "default"):
    """æ€§èƒ½è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # å¼€å§‹è¿½è¸ª
            start_operation_tracking(user_id, operation_type, complexity)
            
            try:
                result = func(*args, **kwargs)
                # æˆåŠŸç»“æŸè¿½è¸ª
                end_operation_tracking(user_id, success=True, result_data={'result_type': type(result).__name__})
                return result
            except Exception as e:
                # å¤±è´¥ç»“æŸè¿½è¸ª
                end_operation_tracking(user_id, success=False, error=str(e))
                raise
        
        return wrapper
    return decorator

if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    import random
    print("ğŸ§ª Starting performance tracker test...")
    
    tracker = UserPerformanceTracker("test_user")
    
    # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    for i in range(5):
        tracker.start_operation("decision_making", complexity=random.randint(1, 5))
        time.sleep(random.uniform(0.1, 0.5))  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        tracker.end_operation(success=random.choice([True, True, True, False]))
    
    # ç”ŸæˆæŠ¥å‘Š
    print(tracker.generate_performance_report())
    
    # å¯¼å‡ºæ•°æ®
    data_file = tracker.export_raw_data('json')
    print(f"Data file: {data_file}")
    
    print("âœ… Test completed!") 