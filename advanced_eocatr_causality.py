"""
é«˜çº§EOCATRå› æœå…³ç³»ç½‘ç»œç³»ç»Ÿ
ä½“ç°å¤æ‚çš„å¤šå±‚å› æœè”ç³»ã€æ¡ä»¶å› æœç½‘ç»œå’Œè§„å¾‹é—´çš„ç›¸äº’ä½œç”¨
"""

import sqlite3
import json
import time
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
import itertools

class AdvancedCausalityType(Enum):
    """é«˜çº§å› æœå…³ç³»ç±»å‹"""
    # åŸºç¡€å› æœç±»å‹
    ENABLING = "enabling"              # ä½¿èƒ½å…³ç³»ï¼šE+O+C â†’ ä½¿å¾— A+T æˆä¸ºå¯èƒ½
    DETERMINISTIC = "deterministic"    # å†³å®šå…³ç³»ï¼šA+T â†’ å¿…ç„¶å¯¼è‡´ R
    PROBABILISTIC = "probabilistic"    # æ¦‚ç‡å…³ç³»ï¼šE+O+C+A+T â†’ æ¦‚ç‡å¯¼è‡´ R
    CONDITIONAL = "conditional"        # æ¡ä»¶å…³ç³»ï¼šå¦‚æœ E+O+Cï¼Œåˆ™ A+T æœ‰æ•ˆ
    INHIBITING = "inhibiting"          # æŠ‘åˆ¶å…³ç³»ï¼šæŸäº›ç»„åˆé˜»æ­¢ç»“æœå‘ç”Ÿ
    
    # é«˜çº§å› æœç±»å‹
    SYNERGISTIC = "synergistic"        # ååŒå…³ç³»ï¼šå¤šä¸ªå…ƒç´ ç»„åˆäº§ç”Ÿå¢å¼ºæ•ˆæœ
    ALTERNATIVE = "alternative"        # æ›¿ä»£å…³ç³»ï¼šå¤šç§è·¯å¾„è¾¾åˆ°åŒæ ·ç»“æœ
    SEQUENTIAL = "sequential"          # åºåˆ—å…³ç³»ï¼šå¿…é¡»æŒ‰ç‰¹å®šé¡ºåºå‘ç”Ÿ
    COMPETITIVE = "competitive"        # ç«äº‰å…³ç³»ï¼šå¤šä¸ªé€‰é¡¹ç›¸äº’æ’æ–¥
    FEEDBACK = "feedback"              # åé¦ˆå…³ç³»ï¼šç»“æœå½±å“å‰ç½®æ¡ä»¶
    EMERGENT = "emergent"              # æ¶Œç°å…³ç³»ï¼šæ•´ä½“æ•ˆæœå¤§äºéƒ¨åˆ†ä¹‹å’Œ

@dataclass
class CausalElement:
    """é«˜çº§å› æœå…ƒç´ """
    element_type: str  # E, O, C, A, T, R
    value: str
    weight: float = 1.0          # åœ¨å› æœå…³ç³»ä¸­çš„æƒé‡
    necessity: float = 1.0       # å¿…è¦æ€§ç¨‹åº¦ 0-1
    activation_threshold: float = 0.5  # æ¿€æ´»é˜ˆå€¼
    temporal_order: int = 0      # æ—¶é—´é¡ºåº (ç”¨äºåºåˆ—å› æœ)
    mutual_exclusions: List[str] = field(default_factory=list)  # äº’æ–¥å…ƒç´ 

@dataclass
class CausalPattern:
    """å› æœæ¨¡å¼"""
    pattern_id: str
    pattern_type: str            # ç®€å•ã€å¤åˆã€ç½‘ç»œ
    causality_relations: List['CausalRelation']
    pattern_strength: float      # æ•´ä½“æ¨¡å¼å¼ºåº¦
    emergence_factor: float = 1.0  # æ¶Œç°å› å­
    
    def get_pattern_complexity(self) -> float:
        """è®¡ç®—æ¨¡å¼å¤æ‚åº¦"""
        base_complexity = len(self.causality_relations)
        # è€ƒè™‘å…³ç³»é—´çš„äº¤äº’
        interaction_complexity = sum([
            rel.get_interaction_complexity() for rel in self.causality_relations
        ])
        return base_complexity + interaction_complexity * 0.1

@dataclass
class CausalRelation:
    """é«˜çº§å› æœå…³ç³»"""
    relation_id: str
    causality_type: AdvancedCausalityType
    antecedent: List[CausalElement]      # å‰ä»¶ï¼ˆåŸå› ï¼‰
    consequent: List[CausalElement]      # åä»¶ï¼ˆç»“æœï¼‰
    strength: float                      # å› æœå…³ç³»å¼ºåº¦ 0-1
    confidence: float                    # ç½®ä¿¡åº¦ 0-1
    support_count: int                   # æ”¯æŒè¯æ®æ•°é‡
    contradiction_count: int             # åé©³è¯æ®æ•°é‡
    
    # é«˜çº§å±æ€§
    temporal_delay: float = 0.0          # æ—¶é—´å»¶è¿Ÿ
    context_dependencies: List[str] = field(default_factory=list)  # ä¸Šä¸‹æ–‡ä¾èµ–
    moderating_factors: Dict[str, float] = field(default_factory=dict)  # è°ƒèŠ‚å› å­
    threshold_conditions: Dict[str, float] = field(default_factory=dict)  # é˜ˆå€¼æ¡ä»¶
    
    def get_interaction_complexity(self) -> float:
        """è·å–äº¤äº’å¤æ‚åº¦"""
        base = len(self.antecedent) * len(self.consequent)
        context_factor = len(self.context_dependencies) * 0.2
        moderation_factor = len(self.moderating_factors) * 0.3
        threshold_factor = len(self.threshold_conditions) * 0.1
        return base + context_factor + moderation_factor + threshold_factor
    
    def evaluate_activation(self, context: Dict[str, Any]) -> float:
        """è¯„ä¼°å› æœå…³ç³»æ¿€æ´»ç¨‹åº¦"""
        # æ£€æŸ¥å‰ä»¶åŒ¹é…
        antecedent_activation = self._evaluate_element_set(self.antecedent, context)
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡ä¾èµ–
        context_satisfaction = self._evaluate_context_dependencies(context)
        
        # åº”ç”¨è°ƒèŠ‚å› å­
        moderation_effect = self._apply_moderating_factors(context)
        
        # æ£€æŸ¥é˜ˆå€¼æ¡ä»¶
        threshold_satisfaction = self._evaluate_threshold_conditions(context)
        
        # ç»¼åˆè®¡ç®—æ¿€æ´»ç¨‹åº¦
        base_activation = antecedent_activation * context_satisfaction
        moderated_activation = base_activation * moderation_effect
        final_activation = moderated_activation * threshold_satisfaction
        
        return min(1.0, final_activation)
    
    def _evaluate_element_set(self, elements: List[CausalElement], context: Dict[str, Any]) -> float:
        """è¯„ä¼°å…ƒç´ é›†åˆåŒ¹é…åº¦"""
        if not elements:
            return 1.0
        
        total_weight = sum(elem.weight for elem in elements)
        if total_weight == 0:
            return 0.0
        
        activated_weight = 0.0
        for element in elements:
            context_value = context.get(element.element_type.lower(), '')
            match_degree = self._calculate_match_degree(element.value, context_value)
            
            if match_degree >= element.activation_threshold:
                activated_weight += element.weight * match_degree
        
        return activated_weight / total_weight
    
    def _calculate_match_degree(self, target_value: str, context_value: str) -> float:
        """è®¡ç®—åŒ¹é…ç¨‹åº¦"""
        if target_value == context_value:
            return 1.0
        elif target_value in context_value or context_value in target_value:
            return 0.8
        elif len(set(target_value.split()) & set(context_value.split())) > 0:
            return 0.6
        else:
            return 0.0
    
    def _evaluate_context_dependencies(self, context: Dict[str, Any]) -> float:
        """è¯„ä¼°ä¸Šä¸‹æ–‡ä¾èµ–æ»¡è¶³åº¦"""
        if not self.context_dependencies:
            return 1.0
        
        satisfied_count = 0
        for dependency in self.context_dependencies:
            if dependency in context and context[dependency]:
                satisfied_count += 1
        
        return satisfied_count / len(self.context_dependencies)
    
    def _apply_moderating_factors(self, context: Dict[str, Any]) -> float:
        """åº”ç”¨è°ƒèŠ‚å› å­"""
        if not self.moderating_factors:
            return 1.0
        
        moderation_effect = 1.0
        for factor, weight in self.moderating_factors.items():
            factor_value = context.get(factor, 0.5)  # é»˜è®¤ä¸­æ€§å€¼
            moderation_effect *= (1.0 + (factor_value - 0.5) * weight)
        
        return max(0.1, min(2.0, moderation_effect))  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    
    def _evaluate_threshold_conditions(self, context: Dict[str, Any]) -> float:
        """è¯„ä¼°é˜ˆå€¼æ¡ä»¶"""
        if not self.threshold_conditions:
            return 1.0
        
        for condition, threshold in self.threshold_conditions.items():
            context_value = context.get(condition, 0.0)
            if context_value < threshold:
                return 0.0  # ä»»ä½•é˜ˆå€¼æ¡ä»¶ä¸æ»¡è¶³éƒ½ä¼šé˜»æ­¢æ¿€æ´»
        
        return 1.0
    
    def to_advanced_expression(self) -> str:
        """è½¬æ¢ä¸ºé«˜çº§è§„å¾‹è¡¨è¾¾å¼"""
        ante_str = " âˆ§ ".join([f"{elem.element_type}:{elem.value}[w:{elem.weight:.1f},n:{elem.necessity:.1f}]" 
                              for elem in self.antecedent])
        cons_str = " âˆ§ ".join([f"{elem.element_type}:{elem.value}[w:{elem.weight:.1f}]" 
                              for elem in self.consequent])
        
        # æ·»åŠ æ—¶é—´å»¶è¿Ÿä¿¡æ¯
        delay_str = f"[å»¶è¿Ÿ:{self.temporal_delay:.1f}s]" if self.temporal_delay > 0 else ""
        
        # æ·»åŠ è°ƒèŠ‚å› å­ä¿¡æ¯
        mod_str = ""
        if self.moderating_factors:
            mod_list = [f"{k}:{v:.1f}" for k, v in self.moderating_factors.items()]
            mod_str = f"[è°ƒèŠ‚:{','.join(mod_list)}]"
        
        # ç”Ÿæˆè¡¨è¾¾å¼
        if self.causality_type == AdvancedCausalityType.SYNERGISTIC:
            return f"({ante_str}) âŠ• ååŒ ({cons_str}){delay_str}{mod_str}"
        elif self.causality_type == AdvancedCausalityType.SEQUENTIAL:
            return f"({ante_str}) âŸ¹ åºåˆ— ({cons_str}){delay_str}{mod_str}"
        elif self.causality_type == AdvancedCausalityType.FEEDBACK:
            return f"({ante_str}) âŸ² åé¦ˆ ({cons_str}){delay_str}{mod_str}"
        elif self.causality_type == AdvancedCausalityType.EMERGENT:
            return f"({ante_str}) âŸ æ¶Œç° ({cons_str}){delay_str}{mod_str}"
        else:
            return f"({ante_str}) â†’[{self.strength:.2f}] ({cons_str}){delay_str}{mod_str}"

class AdvancedEOCATRCausalityNetwork:
    """é«˜çº§EOCATRå› æœå…³ç³»ç½‘ç»œ"""
    
    def __init__(self, network_id: str):
        self.network_id = network_id
        self.causal_patterns: List[CausalPattern] = []
        self.element_registry: Dict[str, Set[str]] = {}  # å…ƒç´ ç±»å‹ -> å€¼é›†åˆ
        self.network_metrics = {
            'total_complexity': 0.0,
            'emergence_potential': 0.0,
            'network_density': 0.0
        }
    
    def add_causal_pattern(self, pattern: CausalPattern):
        """æ·»åŠ å› æœæ¨¡å¼"""
        self.causal_patterns.append(pattern)
        self._update_element_registry(pattern)
        self._update_network_metrics()
    
    def _update_element_registry(self, pattern: CausalPattern):
        """æ›´æ–°å…ƒç´ æ³¨å†Œè¡¨"""
        for relation in pattern.causality_relations:
            for element in relation.antecedent + relation.consequent:
                if element.element_type not in self.element_registry:
                    self.element_registry[element.element_type] = set()
                self.element_registry[element.element_type].add(element.value)
    
    def _update_network_metrics(self):
        """æ›´æ–°ç½‘ç»œæŒ‡æ ‡"""
        if not self.causal_patterns:
            return
        
        # è®¡ç®—æ€»å¤æ‚åº¦
        total_complexity = sum(pattern.get_pattern_complexity() for pattern in self.causal_patterns)
        self.network_metrics['total_complexity'] = total_complexity
        
        # è®¡ç®—æ¶Œç°æ½œåŠ›
        emergence_factors = [pattern.emergence_factor for pattern in self.causal_patterns]
        self.network_metrics['emergence_potential'] = sum(emergence_factors) / len(emergence_factors)
        
        # è®¡ç®—ç½‘ç»œå¯†åº¦
        total_relations = sum(len(pattern.causality_relations) for pattern in self.causal_patterns)
        unique_elements = sum(len(values) for values in self.element_registry.values())
        if unique_elements > 1:
            max_possible_relations = unique_elements * (unique_elements - 1)
            self.network_metrics['network_density'] = total_relations / max_possible_relations
    
    def find_causal_chains(self, start_element: str, end_element: str) -> List[List[CausalRelation]]:
        """æŸ¥æ‰¾å› æœé“¾"""
        chains = []
        visited = set()
        
        def dfs(current_element: str, target_element: str, current_chain: List[CausalRelation]):
            if current_element == target_element and current_chain:
                chains.append(current_chain.copy())
                return
            
            if current_element in visited:
                return
            
            visited.add(current_element)
            
            # æŸ¥æ‰¾ä»¥å½“å‰å…ƒç´ ä¸ºå‰ä»¶çš„å…³ç³»
            for pattern in self.causal_patterns:
                for relation in pattern.causality_relations:
                    for ante_elem in relation.antecedent:
                        if ante_elem.value == current_element:
                            for cons_elem in relation.consequent:
                                current_chain.append(relation)
                                dfs(cons_elem.value, target_element, current_chain)
                                current_chain.pop()
            
            visited.remove(current_element)
        
        dfs(start_element, end_element, [])
        return chains
    
    def evaluate_network_coherence(self) -> float:
        """è¯„ä¼°ç½‘ç»œä¸€è‡´æ€§"""
        if not self.causal_patterns:
            return 0.0
        
        # æ£€æŸ¥å†²çªå…³ç³»
        conflicts = 0
        total_comparisons = 0
        
        for i, pattern1 in enumerate(self.causal_patterns):
            for j, pattern2 in enumerate(self.causal_patterns[i+1:], i+1):
                total_comparisons += 1
                if self._patterns_conflict(pattern1, pattern2):
                    conflicts += 1
        
        if total_comparisons == 0:
            return 1.0
        
        return 1.0 - (conflicts / total_comparisons)
    
    def _patterns_conflict(self, pattern1: CausalPattern, pattern2: CausalPattern) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªæ¨¡å¼æ˜¯å¦å†²çª"""
        # ç®€åŒ–çš„å†²çªæ£€æµ‹ï¼šç›¸åŒå‰ä»¶ï¼Œä¸åŒåä»¶
        for rel1 in pattern1.causality_relations:
            for rel2 in pattern2.causality_relations:
                if self._relations_have_same_antecedent(rel1, rel2):
                    if not self._relations_have_compatible_consequent(rel1, rel2):
                        return True
        return False
    
    def _relations_have_same_antecedent(self, rel1: CausalRelation, rel2: CausalRelation) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå…³ç³»æ˜¯å¦æœ‰ç›¸åŒå‰ä»¶"""
        ante1_values = {f"{elem.element_type}:{elem.value}" for elem in rel1.antecedent}
        ante2_values = {f"{elem.element_type}:{elem.value}" for elem in rel2.antecedent}
        return len(ante1_values & ante2_values) >= 0.7 * min(len(ante1_values), len(ante2_values))
    
    def _relations_have_compatible_consequent(self, rel1: CausalRelation, rel2: CausalRelation) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå…³ç³»æ˜¯å¦æœ‰å…¼å®¹åä»¶"""
        cons1_values = {f"{elem.element_type}:{elem.value}" for elem in rel1.consequent}
        cons2_values = {f"{elem.element_type}:{elem.value}" for elem in rel2.consequent}
        return len(cons1_values & cons2_values) > 0

def create_advanced_causality_from_eocatr(eocatr_data: Dict[str, Any]) -> AdvancedEOCATRCausalityNetwork:
    """ä»EOCATRæ•°æ®åˆ›å»ºé«˜çº§å› æœç½‘ç»œ"""
    network_id = f"network_{eocatr_data.get('rule_id', 'unknown')}"
    network = AdvancedEOCATRCausalityNetwork(network_id)
    
    # æå–åŸºæœ¬ä¿¡æ¯
    environment = eocatr_data.get('environment', '')
    obj = eocatr_data.get('object', '')
    condition = eocatr_data.get('condition', '')
    action = eocatr_data.get('action', '')
    tool = eocatr_data.get('tool', '')
    result = eocatr_data.get('result', '')
    success_rate = eocatr_data.get('success_rate', 0.5)
    
    # åˆ›å»ºé«˜çº§å› æœå…³ç³»
    relations = []
    
    # 1. ååŒå…³ç³»ï¼šç¯å¢ƒå’Œå¯¹è±¡ååŒä½¿èƒ½è¡ŒåŠ¨
    if environment and obj and action:
        synergistic_relation = CausalRelation(
            relation_id=f"{network_id}_synergistic",
            causality_type=AdvancedCausalityType.SYNERGISTIC,
            antecedent=[
                CausalElement("E", environment, weight=1.0, necessity=0.8, activation_threshold=0.6),
                CausalElement("O", obj, weight=1.0, necessity=0.9, activation_threshold=0.7)
            ],
            consequent=[
                CausalElement("A", action, weight=1.0, necessity=1.0)
            ],
            strength=0.85,
            confidence=eocatr_data.get('confidence', 0.5),
            support_count=eocatr_data.get('usage_count', 0),
            contradiction_count=0,
            moderating_factors={'condition_quality': 0.3, 'tool_availability': 0.2}
        )
        relations.append(synergistic_relation)
    
    # 2. åºåˆ—å…³ç³»ï¼šè¡ŒåŠ¨å¿…é¡»åœ¨å·¥å…·å‡†å¤‡åè¿›è¡Œ
    if action and tool and result:
        sequential_relation = CausalRelation(
            relation_id=f"{network_id}_sequential",
            causality_type=AdvancedCausalityType.SEQUENTIAL,
            antecedent=[
                CausalElement("T", tool, weight=1.0, necessity=0.7, temporal_order=1),
                CausalElement("A", action, weight=1.0, necessity=1.0, temporal_order=2)
            ],
            consequent=[
                CausalElement("R", result, weight=1.0, necessity=1.0, temporal_order=3)
            ],
            strength=success_rate,
            confidence=eocatr_data.get('confidence', 0.5),
            support_count=eocatr_data.get('success_count', 0),
            contradiction_count=max(0, eocatr_data.get('usage_count', 0) - eocatr_data.get('success_count', 0)),
            temporal_delay=1.0,
            threshold_conditions={'skill_level': 0.3}
        )
        relations.append(sequential_relation)
    
    # 3. æ¶Œç°å…³ç³»ï¼šå®Œæ•´EOCATRç»„åˆäº§ç”Ÿæ¶Œç°æ•ˆæœ
    if all([environment, obj, condition, action, tool, result]):
        emergent_relation = CausalRelation(
            relation_id=f"{network_id}_emergent",
            causality_type=AdvancedCausalityType.EMERGENT,
            antecedent=[
                CausalElement("E", environment, weight=0.8, necessity=0.6),
                CausalElement("O", obj, weight=1.0, necessity=0.8),
                CausalElement("C", condition, weight=0.6, necessity=0.4),
                CausalElement("A", action, weight=1.0, necessity=1.0),
                CausalElement("T", tool, weight=0.7, necessity=0.5)
            ],
            consequent=[
                CausalElement("R", result, weight=1.0, necessity=1.0)
            ],
            strength=success_rate * 1.2,  # æ¶Œç°æ•ˆåº”å¢å¼º
            confidence=eocatr_data.get('confidence', 0.5),
            support_count=eocatr_data.get('success_count', 0),
            contradiction_count=0,
            context_dependencies=['environmental_stability', 'resource_availability'],
            moderating_factors={'experience_level': 0.4, 'environmental_favorability': 0.3}
        )
        relations.append(emergent_relation)
    
    # åˆ›å»ºå› æœæ¨¡å¼
    pattern = CausalPattern(
        pattern_id=f"pattern_{network_id}",
        pattern_type="å¤åˆæ¶Œç°æ¨¡å¼",
        causality_relations=relations,
        pattern_strength=success_rate,
        emergence_factor=1.3 if len(relations) >= 3 else 1.0
    )
    
    network.add_causal_pattern(pattern)
    return network

def demonstrate_advanced_causality():
    """æ¼”ç¤ºé«˜çº§å› æœå…³ç³»ç½‘ç»œ"""
    print("=== é«˜çº§EOCATRå› æœå…³ç³»ç½‘ç»œæ¼”ç¤º ===\n")
    
    # ä»æ•°æ®åº“åŠ è½½EOCATRè§„å¾‹
    conn = sqlite3.connect('five_libraries/eocatr_rules_final.db')
    cursor = conn.cursor()
    
    cursor.execute('''
    SELECT rule_id, environment, object, condition, action, tool, result, 
           confidence, success_rate, usage_count, success_count
    FROM eocatr_rules LIMIT 2
    ''')
    
    eocatr_rules = cursor.fetchall()
    columns = ['rule_id', 'environment', 'object', 'condition', 'action', 'tool', 'result', 
               'confidence', 'success_rate', 'usage_count', 'success_count']
    
    print("ğŸ•¸ï¸ é«˜çº§EOCATRå› æœå…³ç³»ç½‘ç»œåˆ†æ:")
    
    for rule_data in eocatr_rules:
        rule_dict = dict(zip(columns, rule_data))
        
        print(f"\nğŸ“‹ è§„å¾‹: {rule_dict['rule_id']}")
        print(f"   åŸå§‹EOCATR: E:{rule_dict['environment']}|O:{rule_dict['object']}|C:{rule_dict['condition']}|A:{rule_dict['action']}|T:{rule_dict['tool']}|R:{rule_dict['result']}")
        
        # åˆ›å»ºé«˜çº§å› æœç½‘ç»œ
        network = create_advanced_causality_from_eocatr(rule_dict)
        
        print(f"   é«˜çº§å› æœå…³ç³»ç½‘ç»œ:")
        for pattern in network.causal_patterns:
            print(f"     æ¨¡å¼ç±»å‹: {pattern.pattern_type}")
            print(f"     æ¨¡å¼å¤æ‚åº¦: {pattern.get_pattern_complexity():.2f}")
            print(f"     æ¶Œç°å› å­: {pattern.emergence_factor:.2f}")
            
            for i, relation in enumerate(pattern.causality_relations):
                print(f"       {i+1}. {relation.to_advanced_expression()}")
                
                # æµ‹è¯•æ¿€æ´»
                test_context = {
                    'e': rule_dict['environment'],
                    'o': rule_dict['object'],
                    'c': rule_dict['condition'],
                    'skill_level': 0.7,
                    'experience_level': 0.6,
                    'environmental_favorability': 0.8
                }
                activation = relation.evaluate_activation(test_context)
                print(f"          æ¿€æ´»ç¨‹åº¦: {activation:.2f}")
        
        # ç½‘ç»œæŒ‡æ ‡
        print(f"   ç½‘ç»œæŒ‡æ ‡:")
        for metric, value in network.network_metrics.items():
            print(f"     {metric}: {value:.3f}")
        
        # ç½‘ç»œä¸€è‡´æ€§
        coherence = network.evaluate_network_coherence()
        print(f"     ç½‘ç»œä¸€è‡´æ€§: {coherence:.3f}")
    
    conn.close()
    
    # å±•ç¤ºé«˜çº§å› æœç½‘ç»œçš„ä¼˜åŠ¿
    print(f"\nğŸš€ é«˜çº§å› æœç½‘ç»œçš„çªç ´æ€§ä¼˜åŠ¿:")
    advantages = [
        "ğŸ§  æ™ºèƒ½æ¿€æ´»: æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è®¡ç®—å› æœå…³ç³»æ¿€æ´»ç¨‹åº¦",
        "ğŸ”— å¤åˆå› æœ: æ”¯æŒååŒã€åºåˆ—ã€æ¶Œç°ç­‰å¤æ‚å› æœç±»å‹",
        "â° æ—¶åºå»ºæ¨¡: åŒ…å«æ—¶é—´å»¶è¿Ÿå’Œåºåˆ—çº¦æŸ",
        "ğŸ›ï¸ è°ƒèŠ‚æœºåˆ¶: è€ƒè™‘ç¯å¢ƒå› å­å¯¹å› æœå…³ç³»çš„è°ƒèŠ‚ä½œç”¨",
        "ğŸ—ï¸ ç½‘ç»œç»“æ„: æ„å»ºå®Œæ•´çš„å› æœå…³ç³»ç½‘ç»œè€Œéå­¤ç«‹è§„å¾‹",
        "ğŸ“Š é‡åŒ–è¯„ä¼°: æä¾›ç½‘ç»œå¤æ‚åº¦ã€ä¸€è‡´æ€§ç­‰é‡åŒ–æŒ‡æ ‡",
        "ğŸŒŸ æ¶Œç°æ•ˆåº”: å»ºæ¨¡ç³»ç»Ÿçº§çš„æ¶Œç°è¡Œä¸ºå’Œéçº¿æ€§æ•ˆåº”",
        "ğŸ”„ åé¦ˆå¾ªç¯: æ”¯æŒç»“æœå¯¹å‰ç½®æ¡ä»¶çš„åé¦ˆå½±å“"
    ]
    
    for advantage in advantages:
        print(f"   {advantage}")
    
    print(f"\nğŸ’¡ è¿™ç§è¡¨ç¤ºæ–¹å¼å°†EOCATRä»'ç®€å•å› æœ'å‡çº§ä¸º'æ™ºèƒ½å› æœç½‘ç»œ'ï¼Œ")
    print(f"   çœŸæ­£ä½“ç°äº†è§„å¾‹ä½œä¸º'EOCATRå…ƒç´ é—´ç¨³å®šè”ç³»'çš„æœ¬è´¨ï¼")

if __name__ == "__main__":
    demonstrate_advanced_causality() 