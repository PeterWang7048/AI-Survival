#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨é¢åŸºçº¿å¯¹æ¯”å®éªŒæ¡†æ¶ - æ”¹è¿›ç‰ˆstandaloneå®éªŒ
åŒ…å«çœŸå®åŸºçº¿å®ç°ã€ç»Ÿè®¡åˆ†æã€å¯è§†åŒ–ç­‰å®Œæ•´åŠŸèƒ½
"""

import time
import numpy as np
import random
import json
import os
import csv
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
from collections import defaultdict
import statistics
from scipy import stats

# å¯¼å…¥å¿…è¦ç»„ä»¶
from taxi_environment import StandaloneTaxiEnv
from taxi_ilai_system import TaxiILAISystem

@dataclass
class ExperimentConfig:
    """æ ‡å‡†åŒ–å®éªŒé…ç½®"""
    episodes: int = 50
    max_steps_per_episode: int = 200
    num_runs: int = 3  # å¤šæ¬¡è¿è¡Œæ±‚å¹³å‡
    random_seed: int = 42
    clear_libraries: bool = True
    detailed_logging: bool = False
    save_individual_results: bool = True
    statistical_analysis: bool = True

@dataclass
class AgentMetrics:
    """æ™ºèƒ½ä½“è¯¦ç»†æŒ‡æ ‡"""
    name: str
    success_rate: float
    success_rate_std: float
    avg_reward: float
    reward_std: float
    avg_steps: float
    steps_std: float
    avg_pickup_rate: float
    avg_dropoff_rate: float
    convergence_episode: int  # æ€§èƒ½æ”¶æ•›çš„å›åˆæ•°
    execution_time: float
    memory_usage: float
    decision_distribution: Dict[str, int]  # åŠ¨ä½œåˆ†å¸ƒ
    learning_curve: List[float]  # å­¦ä¹ æ›²çº¿
    confidence_interval_95: Tuple[float, float]  # æˆåŠŸç‡95%ç½®ä¿¡åŒºé—´

class RealRandomAgent:
    """çœŸå®éšæœºæ™ºèƒ½ä½“"""
    def __init__(self):
        self.name = "Random Agent"
        self.action_space_size = 6
    
    def select_action(self, state: int) -> int:
        return random.randint(0, 5)
    
    def learn_from_outcome(self, state, action, next_state, reward, done):
        pass  # éšæœºæ™ºèƒ½ä½“ä¸å­¦ä¹ 

class RealRuleBasedAgent:
    """çœŸå®è§„åˆ™æ™ºèƒ½ä½“"""
    def __init__(self):
        self.name = "Rule-Based Agent"
        self.locs = [(0, 0), (0, 4), (4, 0), (4, 3)]
    
    def select_action(self, state: int) -> int:
        """åŸºäºå¯å‘å¼è§„åˆ™é€‰æ‹©åŠ¨ä½œ"""
        # è§£ç çŠ¶æ€
        taxi_row = state // 100
        state_remain = state % 100
        taxi_col = state_remain // 20
        state_remain = state_remain % 20
        passenger_location = state_remain // 4
        destination = state_remain % 4
        
        taxi_pos = (taxi_row, taxi_col)
        
        # é˜¶æ®µ1ï¼šå»æ¥å®¢
        if passenger_location < 4:
            passenger_pos = self.locs[passenger_location]
            if taxi_pos == passenger_pos:
                return 4  # pickup
            
            # A*å¼è·¯å¾„è§„åˆ’ï¼ˆç®€åŒ–ç‰ˆï¼‰
            return self._move_towards_target(taxi_pos, passenger_pos)
        
        # é˜¶æ®µ2ï¼šé€å®¢
        else:
            dest_pos = self.locs[destination]
            if taxi_pos == dest_pos:
                return 5  # dropoff
            
            return self._move_towards_target(taxi_pos, dest_pos)
    
    def _move_towards_target(self, current: Tuple[int, int], target: Tuple[int, int]) -> int:
        """æœç›®æ ‡ç§»åŠ¨ï¼ˆè€ƒè™‘å¢™å£ï¼‰"""
        curr_row, curr_col = current
        targ_row, targ_col = target
        
        # ä¼˜å…ˆå¤„ç†è¡Œç§»åŠ¨
        if curr_row < targ_row:
            return 0  # å—
        elif curr_row > targ_row:
            return 1  # åŒ—
        # å†å¤„ç†åˆ—ç§»åŠ¨
        elif curr_col < targ_col:
            return 2  # ä¸œ
        elif curr_col > targ_col:
            return 3  # è¥¿
        
        # å·²åˆ°è¾¾ç›®æ ‡ä½ç½®
        return random.randint(0, 3)
    
    def learn_from_outcome(self, state, action, next_state, reward, done):
        pass  # è§„åˆ™æ™ºèƒ½ä½“ä¸å­¦ä¹ 

class SimpleQLearningAgent:
    """ä¼˜åŒ–çš„Q-Learningæ™ºèƒ½ä½“"""
    def __init__(self, learning_rate=0.3, discount_factor=0.95, epsilon=0.3):
        self.name = "Q-Learning Agent (Optimized)"
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon  # åˆå§‹æ¢ç´¢ç‡
        self.epsilon_decay = 0.995  # æ¢ç´¢ç‡è¡°å‡
        self.epsilon_min = 0.01     # æœ€å°æ¢ç´¢ç‡
        self.q_table = defaultdict(lambda: defaultdict(float))
        self.action_space_size = 6
        self.episode_count = 0
    
    def select_action(self, state: int) -> int:
        """ä¼˜åŒ–çš„Îµ-è´ªå¿ƒç­–ç•¥"""
        if random.random() < self.epsilon:
            return random.randint(0, 5)
        
        # é€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
        q_values = [self.q_table[state][a] for a in range(self.action_space_size)]
        return q_values.index(max(q_values))
    
    def learn_from_outcome(self, state, action, next_state, reward, done):
        """ä¼˜åŒ–çš„Q-Learningæ›´æ–°"""
        # Q-Learningæ›´æ–°
        if done:
            target = reward
        else:
            next_q_values = [self.q_table[next_state][a] for a in range(self.action_space_size)]
            target = reward + self.discount_factor * max(next_q_values)
        
        current_q = self.q_table[state][action]
        self.q_table[state][action] = current_q + self.learning_rate * (target - current_q)
        
        # æ¢ç´¢ç‡è¡°å‡
        if done:
            self.episode_count += 1
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

class TaxiAStarAgent:
    """A*æœç´¢æ™ºèƒ½ä½“ - Taxiç¯å¢ƒä¸“ç”¨ (ç®€åŒ–ç¨³å®šç‰ˆ)"""
    
    def __init__(self, library_directory=None):
        self.name = "A* Search Agent"
        self.grid_size = 5
        self.special_locations = {
            'R': (0, 0), 'G': (0, 4), 'Y': (4, 0), 'B': (4, 3)
        }
                
    def _decode_state(self, state):
        """è§£ç TaxiçŠ¶æ€"""
        destination_idx = state % 4
        state //= 4
        passenger_idx = state % 5
        state //= 5
        taxi_col = state % 5
        taxi_row = state // 5
        return taxi_row, taxi_col, passenger_idx, destination_idx
    
    def _get_location_pos(self, location_idx):
        """æ ¹æ®ä½ç½®ç´¢å¼•è·å–åæ ‡"""
        if location_idx < 4:
            return list(self.special_locations.values())[location_idx]
        return (-1, -1)  # ä¹˜å®¢åœ¨è½¦ä¸Š

    def _manhattan_distance(self, pos1, pos2):
        """è®¡ç®—æ›¼å“ˆé¡¿è·ç¦»"""
        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])
    
    def select_action(self, state):
        """ç®€åŒ–A*å†³ç­–ç®—æ³•"""
        taxi_row, taxi_col, passenger_idx, destination_idx = self._decode_state(state)
        taxi_pos = (taxi_row, taxi_col)
        
        # ç‰¹æ®ŠåŠ¨ä½œæ£€æŸ¥
        if passenger_idx == 4:  # ä¹˜å®¢åœ¨è½¦ä¸Š
            destination_pos = self._get_location_pos(destination_idx)
            if taxi_pos == destination_pos:
                return 5  # dropoff
        else:  # ä¹˜å®¢ä¸åœ¨è½¦ä¸Š
            passenger_pos = self._get_location_pos(passenger_idx)
            if taxi_pos == passenger_pos:
                return 4  # pickup
        
        # ç®€åŒ–çš„æ–¹å‘é€‰æ‹©
        if passenger_idx == 4:  # å·²è½½å®¢ï¼Œå‰å¾€ç›®æ ‡
            target_pos = self._get_location_pos(destination_idx)
        else:  # æœªè½½å®¢ï¼Œå‰å¾€ä¹˜å®¢ä½ç½®
            target_pos = self._get_location_pos(passenger_idx)
        
        # è´ªå¿ƒé€‰æ‹©æœ€ä½³æ–¹å‘
        best_action = 0
        best_distance = float('inf')
        
        # å°è¯•å››ä¸ªç§»åŠ¨æ–¹å‘
        moves = [(1, 0, 0), (-1, 0, 1), (0, 1, 2), (0, -1, 3)]  # (dr, dc, action)
        for dr, dc, action in moves:
            new_row, new_col = taxi_row + dr, taxi_col + dc
            
            # æ£€æŸ¥è¾¹ç•Œ
            if 0 <= new_row < 5 and 0 <= new_col < 5:
                distance = self._manhattan_distance((new_row, new_col), target_pos)
                if distance < best_distance:
                    best_distance = distance
                    best_action = action
        
        return best_action
    
    def learn_from_outcome(self, state, action, next_state, reward, done):
        """A*ä¸éœ€è¦å­¦ä¹ """
        pass

class TaxiDQNAgent:
    """ä¼˜åŒ–çš„æ·±åº¦Qç½‘ç»œæ™ºèƒ½ä½“"""
    
    def __init__(self, library_directory=None):
        self.name = "Deep Q Network (Optimized)"
        self.epsilon = 0.3        # å¢åŠ åˆå§‹æ¢ç´¢ç‡
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.learning_rate = 0.1  # å¢åŠ å­¦ä¹ ç‡
        self.discount_factor = 0.95
        self.action_space_size = 6
        
        # ç®€åŒ–çš„DQNï¼šä½¿ç”¨Qå€¼ä¼°è®¡æ¨¡æ‹Ÿç¥ç»ç½‘ç»œ
        self.q_estimates = {}
        self.training_step = 0
        self.episode_count = 0
        
    def _decode_state(self, state):
        """è§£ç TaxiçŠ¶æ€"""
        destination_idx = state % 4
        state //= 4
        passenger_idx = state % 5
        state //= 5
        taxi_col = state % 5
        taxi_row = state // 5
        return taxi_row, taxi_col, passenger_idx, destination_idx
    
    def _estimate_q_value(self, state, action):
        """ä¼°è®¡Qå€¼ï¼ˆç®€åŒ–çš„ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿï¼‰"""
        key = (state, action)
        
        if key in self.q_estimates:
            return self.q_estimates[key]
        
        # åŸºäºå¯å‘å¼ä¼°è®¡åˆå§‹Qå€¼
        taxi_row, taxi_col, passenger_idx, destination_idx = self._decode_state(state)
        
        # åŸºç¡€å¯å‘å¼Qå€¼ä¼°è®¡
        base_q = 0.0
        
        if action == 4:  # pickup
            if passenger_idx != 4:
                passenger_pos = self._get_location_pos(passenger_idx)
                base_q = 10.0 if (taxi_row, taxi_col) == passenger_pos else -5.0
            else:
                base_q = -10.0
        elif action == 5:  # dropoff
            if passenger_idx == 4:
                destination_pos = self._get_location_pos(destination_idx)
                base_q = 20.0 if (taxi_row, taxi_col) == destination_pos else -5.0
            else:
                base_q = -10.0
        else:  # ç§»åŠ¨åŠ¨ä½œ (0-3)
            # ç®€åŒ–ä¸ºéšæœºåˆå§‹åŒ–
            base_q = random.uniform(-2.0, 5.0)
        
        # æ·»åŠ å™ªå£°æ¨¡æ‹Ÿç¥ç»ç½‘ç»œçš„ä¸ç¡®å®šæ€§
        noise = random.uniform(-1.0, 1.0)
        estimated_q = base_q + noise
        
        self.q_estimates[key] = estimated_q
        return estimated_q
    
    def _get_location_pos(self, location_idx):
        """è·å–ä½ç½®åæ ‡"""
        locations = [(0, 0), (0, 4), (4, 0), (4, 3)]
        if location_idx < 4:
            return locations[location_idx]
        return (-1, -1)
    
    def select_action(self, state):
        """Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        # Îµ-è´ªå¿ƒç­–ç•¥
        if random.random() < self.epsilon:
            return random.randint(0, 5)
        
        # é€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
        q_values = [self._estimate_q_value(state, action) for action in range(self.action_space_size)]
        max_q = max(q_values)
        best_actions = [action for action, q in enumerate(q_values) if q == max_q]
        return random.choice(best_actions)
    
    def learn_from_outcome(self, state, action, next_state, reward, done):
        """æ ‡å‡†DQNå­¦ä¹ è¿‡ç¨‹"""
        self.training_step += 1
        
        # DQNç›®æ ‡å€¼è®¡ç®—
        if done:
            target = reward
        else:
            # è®¡ç®—ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
            next_q_values = [self._estimate_q_value(next_state, a) for a in range(self.action_space_size)]
            target = reward + self.discount_factor * max(next_q_values)
        
        # æ›´æ–°å½“å‰çŠ¶æ€-åŠ¨ä½œçš„Qå€¼
        key = (state, action)
        current_q = self.q_estimates.get(key, 0.0)
        self.q_estimates[key] = current_q + self.learning_rate * (target - current_q)
        
        # æ¢ç´¢ç‡è¡°å‡ï¼ˆæ¯episodeç»“æŸæ—¶ï¼‰
        if done:
            self.episode_count += 1
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

class ComprehensiveBaselineExperiment:
    """å…¨é¢åŸºçº¿å¯¹æ¯”å®éªŒ"""
    
    def __init__(self, config: ExperimentConfig = None):
        self.config = config or ExperimentConfig()
        self.results = {}
        self.raw_data = {}  # å­˜å‚¨åŸå§‹æ•°æ®ç”¨äºç»Ÿè®¡åˆ†æ
        
        # å®éªŒåŸºç¡€ä¿¡æ¯
        self.base_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_run_id = 0
        self.current_run_log_file = None
        
        # è®¾ç½®éšæœºç§å­
        random.seed(self.config.random_seed)
        np.random.seed(self.config.random_seed)
        
        print("ğŸš€ **å…¨é¢åŸºçº¿å¯¹æ¯”å®éªŒæ¡†æ¶**")
        print(f"ğŸ“Š é…ç½®: {self.config.episodes}å›åˆ x {self.config.num_runs}æ¬¡è¿è¡Œ")
        print(f"âš™ï¸ æœ€å¤§æ­¥æ•°: {self.config.max_steps_per_episode}, éšæœºç§å­: {self.config.random_seed}")
        print("=" * 70)
    
    def log(self, message: str):
        """å†™å…¥æ—¥å¿—æ–‡ä»¶å’Œæ§åˆ¶å°"""
        if self.current_run_log_file:
            self.current_run_log_file.write(f"{message}\n")
            self.current_run_log_file.flush()
        print(message)
    
    def _create_run_log_file(self, run_id: int):
        """ä¸ºæ¯æ¬¡è¿è¡Œåˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰æ™ºèƒ½ä½“"""
        if self.config.detailed_logging:
            log_filename = f"000log/taxi-{self.base_timestamp}-run{run_id:02d}.log"
            self.current_run_log_file = open(log_filename, 'w', encoding='utf-8')
            self.current_run_id = run_id
            self.log(f"ğŸš• TaxiåŸºçº¿å¯¹æ¯”å®éªŒè¯¦ç»†æ—¥å¿—")
            self.log(f"è¿è¡ŒID: {run_id:02d}/{self.config.num_runs}")
            self.log(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self.log(f"å®éªŒé…ç½®: {self.config.episodes}å›åˆ Ã— 6ä¸ªåŸºçº¿æ™ºèƒ½ä½“")
            self.log("=" * 80)
            return log_filename
        return None
    
    def _close_run_log_file(self):
        """å…³é—­å½“å‰è¿è¡Œçš„æ—¥å¿—æ–‡ä»¶"""
        if self.current_run_log_file:
            self.log(f"è¿è¡Œç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            self.log("=" * 80)
            self.current_run_log_file.close()
            self.current_run_log_file = None
    
    def run_agent(self, agent_class, agent_name: str, library_suffix: str = "") -> AgentMetrics:
        """è¿è¡Œå•ä¸ªæ™ºèƒ½ä½“"""
        print(f"\nğŸ¤– æµ‹è¯• {agent_name}...")
        self.log(f"\n=== {agent_name} å®éªŒå¼€å§‹ ===")
        self.log(f"é…ç½®: {self.config.episodes}å›åˆ Ã— {self.config.num_runs}æ¬¡è¿è¡Œ")
        
        all_run_results = []
        all_learning_curves = []
        decision_counts = defaultdict(int)
        
        start_time = time.time()
        
        for run in range(self.config.num_runs):
            # ä¸ºå½“å‰è¿è¡Œåˆ›å»ºç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
            if self.config.detailed_logging:
                print(f"   ğŸ“ è¿è¡Œ {run+1}/{self.config.num_runs}")
                self._create_run_log_file(agent_name, run+1)
            
            # åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹
            if agent_class == TaxiILAISystem:
                agent = agent_class(f"baseline_test_{library_suffix}_{run}")
                if self.config.clear_libraries:
                    agent.five_libraries.clear_all_data()
            else:
                agent = agent_class()
            
            run_results, learning_curve, decisions = self._run_single_agent_run(agent, run)
            all_run_results.extend(run_results)
            all_learning_curves.append(learning_curve)
            
            # ç´¯ç§¯å†³ç­–ç»Ÿè®¡
            for action, count in decisions.items():
                decision_counts[action] += count
            
            # å…³é—­å½“å‰è¿è¡Œçš„æ—¥å¿—æ–‡ä»¶
            if self.config.detailed_logging:
                self._close_run_log_file()
        
        execution_time = time.time() - start_time
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self._calculate_comprehensive_metrics(
            agent_name, all_run_results, all_learning_curves, 
            decision_counts, execution_time
        )
        
        print(f"   âœ… {agent_name} å®Œæˆ: æˆåŠŸç‡ {metrics.success_rate:.1f}% Â± {metrics.success_rate_std:.1f}%")
        return metrics
    
    def _run_single_agent_run(self, agent, run_id: int) -> Tuple[List[Dict], List[float], Dict[str, int]]:
        """è¿è¡Œå•ä¸ªæ™ºèƒ½ä½“çš„ä¸€æ¬¡å®Œæ•´è¿è¡Œ"""
        env = StandaloneTaxiEnv()
        results = []
        learning_curve = []
        decision_counts = defaultdict(int)
        
        # æ»‘åŠ¨çª—å£è®¡ç®—å­¦ä¹ æ›²çº¿
        window_size = min(10, self.config.episodes // 4)
        success_window = []
        
        for episode in range(self.config.episodes):
            state = env.reset()
            total_reward = 0
            steps = 0
            pickup_success = False
            dropoff_success = False
            
            # è¯¦ç»†æ—¥å¿—è®°å½• - å›åˆå¼€å§‹
            if self.config.detailed_logging:
                self.log(f"  ğŸ“ å›åˆ {episode+1}/{self.config.episodes} å¼€å§‹ - åˆå§‹çŠ¶æ€: {state}")
            
            for step in range(self.config.max_steps_per_episode):
                # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œ
                if isinstance(agent, TaxiILAISystem):
                    decision_result = agent.make_decision(state)
                    action_name = decision_result.selected_action
                    action_map = {"ä¸‹": 0, "ä¸Š": 1, "å³": 2, "å·¦": 3, "pickup": 4, "dropoff": 5}
                    action = action_map.get(action_name, 0)
                    decision_counts[action_name] += 1
                    
                    # è¯¦ç»†æ—¥å¿—è®°å½• - ILAIç³»ç»Ÿ
                    if self.config.detailed_logging:
                        self.log(f"    æ­¥éª¤{step+1}: çŠ¶æ€{state} â†’ é€‰æ‹©åŠ¨ä½œ[{action_name}] (æ¨ç†: {getattr(decision_result, 'reasoning', 'åŸºäºBMP')})")
                        
                else:
                    action = agent.select_action(state)
                    action_names = {0: "ä¸‹", 1: "ä¸Š", 2: "å³", 3: "å·¦", 4: "pickup", 5: "dropoff"}
                    action_name = action_names[action]
                    decision_counts[action_name] += 1
                    
                    # è¯¦ç»†æ—¥å¿—è®°å½• - å…¶ä»–æ™ºèƒ½ä½“
                    if self.config.detailed_logging:
                        agent_type = agent.__class__.__name__
                        if hasattr(agent, 'name'):
                            agent_type = agent.name
                        self.log(f"    æ­¥éª¤{step+1}: çŠ¶æ€{state} â†’ {agent_type}é€‰æ‹©åŠ¨ä½œ[{action_name}]")
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, _ = env.step(action)
                
                # è¯¦ç»†æ—¥å¿—è®°å½• - åŠ¨ä½œç»“æœ
                if self.config.detailed_logging:
                    status = "æˆåŠŸ" if done and reward > 0 else "ç»§ç»­" if not done else "å¤±è´¥"
                    self.log(f"        â†’ ç»“æœ: æ–°çŠ¶æ€{next_state}, å¥–åŠ±{reward:+.1f}, {status}")
                    
                    # ç‰¹æ®Šäº‹ä»¶è®°å½•
                    if action == 4:  # pickup
                        if reward != -10:
                            self.log(f"        âœ… æˆåŠŸæ¥å®¢! (å¥–åŠ±: {reward:+.1f})")
                        else:
                            self.log(f"        âŒ æ¥å®¢å¤±è´¥! (å¥–åŠ±: {reward:+.1f})")
                    elif action == 5:  # dropoff
                        if reward == 20:
                            self.log(f"        ğŸ¯ æˆåŠŸé€è¾¾! ä»»åŠ¡å®Œæˆ! (å¥–åŠ±: {reward:+.1f})")
                        else:
                            self.log(f"        âŒ é€å®¢å¤±è´¥! (å¥–åŠ±: {reward:+.1f})")
                
                # æ™ºèƒ½ä½“å­¦ä¹ 
                if isinstance(agent, TaxiILAISystem):
                    agent.learn_from_outcome(state, action_name, next_state, reward, done)
                else:
                    agent.learn_from_outcome(state, action, next_state, reward, done)
                
                total_reward += reward
                steps += 1
                
                # è®°å½•å…³é”®äº‹ä»¶
                if action == 4 and reward != -10:  # æˆåŠŸæ¥å®¢
                    pickup_success = True
                if action == 5 and reward == 20:   # æˆåŠŸé€å®¢
                    dropoff_success = True
                
                if done:
                    break
                    
                state = next_state
            
            # è®°å½•ç»“æœ
            success = done and reward > 0
            success_window.append(success)
            
            if len(success_window) > window_size:
                success_window.pop(0)
            
            # è®¡ç®—å½“å‰çª—å£æˆåŠŸç‡ä½œä¸ºå­¦ä¹ æ›²çº¿ç‚¹
            current_success_rate = sum(success_window) / len(success_window) * 100
            learning_curve.append(current_success_rate)
            
            # è¯¦ç»†æ—¥å¿—è®°å½• - å›åˆç»“æŸ
            if self.config.detailed_logging:
                status_icon = "ğŸ¯" if success else "âŒ"
                pickup_status = "âœ…æ¥å®¢" if pickup_success else "âŒæ¥å®¢"
                dropoff_status = "âœ…é€è¾¾" if dropoff_success else "âŒé€è¾¾"
                self.log(f"  {status_icon} å›åˆ {episode+1} ç»“æŸ: {pickup_status}, {dropoff_status}, "
                        f"æ€»å¥–åŠ±{total_reward:+.1f}, {steps}æ­¥, å½“å‰æˆåŠŸç‡{current_success_rate:.1f}%")
            
            results.append({
                'episode': episode,
                'success': success,
                'reward': total_reward,
                'steps': steps,
                'pickup_success': pickup_success,
                'dropoff_success': dropoff_success,
                'run_id': run_id
            })
        
        return results, learning_curve, dict(decision_counts)
    
    def _calculate_comprehensive_metrics(self, name: str, results: List[Dict], 
                                       learning_curves: List[List[float]], 
                                       decisions: Dict[str, int], 
                                       execution_time: float) -> AgentMetrics:
        """è®¡ç®—å…¨é¢çš„æ™ºèƒ½ä½“æŒ‡æ ‡"""
        
        # åŸºç¡€ç»Ÿè®¡
        successes = [r['success'] for r in results]
        rewards = [r['reward'] for r in results]
        steps = [r['steps'] for r in results]
        pickup_successes = [r['pickup_success'] for r in results]
        dropoff_successes = [r['dropoff_success'] for r in results]
        
        success_rate = np.mean(successes) * 100
        success_rate_std = np.std(successes) * 100
        
        # è®¡ç®—æ”¶æ•›ç‚¹ï¼ˆå­¦ä¹ æ›²çº¿ç¨³å®šçš„å›åˆï¼‰
        convergence_episode = self._find_convergence_point(learning_curves)
        
        # 95%ç½®ä¿¡åŒºé—´
        if len(successes) > 1:
            confidence_interval = stats.t.interval(
                0.95, len(successes)-1, 
                loc=np.mean(successes), 
                scale=stats.sem(successes)
            )
            confidence_interval = (confidence_interval[0] * 100, confidence_interval[1] * 100)
        else:
            confidence_interval = (success_rate, success_rate)
        
        # å¹³å‡å­¦ä¹ æ›²çº¿
        if learning_curves:
            avg_learning_curve = []
            max_len = max(len(curve) for curve in learning_curves)
            for i in range(max_len):
                point_values = [curve[i] for curve in learning_curves if i < len(curve)]
                avg_learning_curve.append(np.mean(point_values))
        else:
            avg_learning_curve = []
        
        return AgentMetrics(
            name=name,
            success_rate=success_rate,
            success_rate_std=success_rate_std,
            avg_reward=np.mean(rewards),
            reward_std=np.std(rewards),
            avg_steps=np.mean(steps),
            steps_std=np.std(steps),
            avg_pickup_rate=np.mean(pickup_successes) * 100,
            avg_dropoff_rate=np.mean(dropoff_successes) * 100,
            convergence_episode=convergence_episode,
            execution_time=execution_time,
            memory_usage=0.0,  # ç®€åŒ–ç‰ˆæœ¬æš‚ä¸å®ç°
            decision_distribution=decisions,
            learning_curve=avg_learning_curve,
            confidence_interval_95=confidence_interval
        )
    
    def _find_convergence_point(self, learning_curves: List[List[float]]) -> int:
        """å¯»æ‰¾å­¦ä¹ æ›²çº¿æ”¶æ•›ç‚¹"""
        if not learning_curves:
            return -1
        
        # è®¡ç®—å¹³å‡å­¦ä¹ æ›²çº¿
        max_len = max(len(curve) for curve in learning_curves)
        avg_curve = []
        for i in range(max_len):
            point_values = [curve[i] for curve in learning_curves if i < len(curve)]
            avg_curve.append(np.mean(point_values))
        
        if len(avg_curve) < 10:
            return len(avg_curve)
        
        # å¯»æ‰¾æ”¶æ•›ç‚¹ï¼šè¿ç»­5ä¸ªç‚¹çš„æ–¹å·®å°äºé˜ˆå€¼
        window_size = 5
        variance_threshold = 25  # æˆåŠŸç‡æ–¹å·®é˜ˆå€¼
        
        for i in range(window_size, len(avg_curve)):
            window = avg_curve[i-window_size:i]
            if np.var(window) < variance_threshold:
                return i - window_size
        
        return len(avg_curve) // 2  # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ä¸­ç‚¹
    
    def run_complete_comparison(self):
        """è¿è¡Œå®Œæ•´å¯¹æ¯”å®éªŒ"""
        print("\nğŸ† **å¼€å§‹å…¨é¢åŸºçº¿å¯¹æ¯”å®éªŒ**")
        
        # å®šä¹‰æ‰€æœ‰æ™ºèƒ½ä½“
        agents = [
            (TaxiILAISystem, "ILAI System", "ilai"),
            (TaxiAStarAgent, "A* Search Agent", "astar"),
            (RealRuleBasedAgent, "Rule-Based Agent", "rule"),
            (TaxiDQNAgent, "Deep Q Network", "dqn"),
            (SimpleQLearningAgent, "Q-Learning Agent", "qlearn"),
            (RealRandomAgent, "Random Agent", "random")
        ]
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨ - æ¯ä¸ªæ™ºèƒ½ä½“å­˜å‚¨å¤šä¸ªrunçš„ç»“æœ
        all_agents_results = {agent_name: [] for _, agent_name, _ in agents}
        
        # æŒ‰runç»„ç»‡å®éªŒï¼šæ¯ä¸ªrunåŒ…å«æ‰€æœ‰æ™ºèƒ½ä½“
        for run in range(self.config.num_runs):
            # åˆ›å»ºè¯¥runçš„æ—¥å¿—æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰æ™ºèƒ½ä½“ï¼‰
            if self.config.detailed_logging:
                self._create_run_log_file(run + 1)
            
            print(f"\nğŸ“ è¿è¡Œ {run+1}/{self.config.num_runs}")
            if self.config.detailed_logging:
                self.log(f"\n{'='*60}")
                self.log(f"ğŸš€ è¿è¡Œ {run+1}/{self.config.num_runs} å¼€å§‹")
                self.log(f"{'='*60}")
            
            # åœ¨è¯¥runä¸­ä¾æ¬¡è¿è¡Œæ‰€æœ‰æ™ºèƒ½ä½“
            for agent_class, agent_name, suffix in agents:
                try:
                    if self.config.detailed_logging:
                        self.log(f"\n=== {agent_name} å¼€å§‹ ===")
                        self.log(f"é…ç½®: {self.config.episodes}å›åˆ")
                    
                    # åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹
                    if agent_class == TaxiILAISystem:
                        agent = agent_class(f"run_{run+1}_{suffix}")
                        if self.config.clear_libraries:
                            agent.five_libraries.clear_all_data()
                    else:
                        agent = agent_class()
                    
                    # è¿è¡Œè¯¥æ™ºèƒ½ä½“çš„å®éªŒ
                    run_results, learning_curve, decisions = self._run_single_agent_run(agent, run+1)
                    
                    # è®¡ç®—è¯¥runçš„æŒ‡æ ‡
                    success_count = sum(1 for r in run_results if r['success'])
                    success_rate = success_count / len(run_results) if run_results else 0
                    avg_reward = sum(r['reward'] for r in run_results) / len(run_results) if run_results else 0
                    avg_steps = sum(r['steps'] for r in run_results) / len(run_results) if run_results else 0
                    
                    # å­˜å‚¨è¯¥runçš„ç»“æœ
                    run_metrics = {
                        'run_id': run + 1,
                        'success_rate': success_rate,
                        'success_count': success_count,
                        'total_episodes': len(run_results),
                        'avg_reward': avg_reward,
                        'avg_steps': avg_steps,
                        'learning_curve': learning_curve,
                        'decisions': dict(decisions),
                        'raw_results': run_results
                    }
                    
                    all_agents_results[agent_name].append(run_metrics)
                    
                    if self.config.detailed_logging:
                        self.log(f"=== {agent_name} å®Œæˆ: æˆåŠŸç‡{success_rate:.1%} ({success_count}/{len(run_results)}) ===")
                    print(f"   âœ… {agent_name}: æˆåŠŸç‡{success_rate:.1%}")
                    
                except Exception as e:
                    if self.config.detailed_logging:
                        self.log(f"âŒ {agent_name} å‡ºé”™: {str(e)}")
                    print(f"   âŒ {agent_name} å¤±è´¥: {str(e)}")
                    
                    # æ·»åŠ ç©ºç»“æœä»¥ä¿æŒä¸€è‡´æ€§
                    all_agents_results[agent_name].append({
                        'run_id': run + 1,
                        'success_rate': 0,
                        'success_count': 0,
                        'total_episodes': 0,
                        'avg_reward': 0,
                        'avg_steps': 0,
                        'learning_curve': [],
                        'decisions': {},
                        'raw_results': []
                    })
            
            # å…³é—­è¯¥runçš„æ—¥å¿—æ–‡ä»¶
            if self.config.detailed_logging:
                self._close_run_log_file()
        
        # è®¡ç®—æ‰€æœ‰æ™ºèƒ½ä½“çš„æœ€ç»ˆç»Ÿè®¡æŒ‡æ ‡
        for agent_name in all_agents_results:
            agent_runs = all_agents_results[agent_name]
            if agent_runs:
                # è®¡ç®—è·¨æ‰€æœ‰runsçš„ç»Ÿè®¡æŒ‡æ ‡
                success_rates = [r['success_rate'] for r in agent_runs]
                avg_rewards = [r['avg_reward'] for r in agent_runs]
                avg_steps_list = [r['avg_steps'] for r in agent_runs]
                
                # æ”¶é›†æ‰€æœ‰episodesçš„åŸå§‹æ•°æ®
                all_episodes = []
                for run_data in agent_runs:
                    all_episodes.extend(run_data['raw_results'])
                
                if all_episodes:
                    metrics = self._calculate_comprehensive_metrics(
                        agent_name, all_episodes, [r['learning_curve'] for r in agent_runs],
                        {}, 0  # decisionså’Œexecution_timeåœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ä¸é‡è¦
                    )
                    self.results[agent_name] = metrics
                
                self.raw_data[agent_name] = agent_runs  # å­˜å‚¨è¯¦ç»†çš„runæ•°æ®
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        # ä¿å­˜ç»“æœ
        if self.config.save_individual_results:
            self.save_detailed_results()
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢çš„å¯¹æ¯”æŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ— ç»“æœæ•°æ®å¯ç”ŸæˆæŠ¥å‘Š")
            return
        
        print(f"\nğŸ“Š **å®éªŒç»“æœåˆ†ææŠ¥å‘Š**")
        print("=" * 80)
        
        # 1. æ€§èƒ½æ’å
        sorted_agents = sorted(self.results.items(), key=lambda x: x[1].success_rate, reverse=True)
        
        print(f"\nğŸ† **æ€§èƒ½æ’å** (æŒ‰æˆåŠŸç‡)")
        print("-" * 80)
        print(f"{'æ’å':<4} {'æ™ºèƒ½ä½“':<20} {'æˆåŠŸç‡':<15} {'å¹³å‡å¥–åŠ±':<12} {'å¹³å‡æ­¥æ•°':<10} {'æ”¶æ•›å›åˆ':<10}")
        print("-" * 80)
        
        for i, (name, metrics) in enumerate(sorted_agents, 1):
            print(f"{i:<4} {name:<20} {metrics.success_rate:.1f}% Â± {metrics.success_rate_std:.1f} "
                  f"{metrics.avg_reward:>+7.1f} {metrics.avg_steps:>8.1f} {metrics.convergence_episode:>8d}")
        
        # 2. ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ
        if self.config.statistical_analysis and len(self.results) >= 2:
            print(f"\nğŸ“ˆ **ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ**")
            print("-" * 50)
            self._perform_statistical_analysis(sorted_agents)
        
        # 3. è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
        print(f"\nğŸ“‹ **è¯¦ç»†æ€§èƒ½æŒ‡æ ‡**")
        print("-" * 80)
        
        for name, metrics in sorted_agents:
            print(f"\nğŸ¤– **{name}**:")
            print(f"   ğŸ“Š æˆåŠŸç‡: {metrics.success_rate:.1f}% Â± {metrics.success_rate_std:.1f}%")
            print(f"   ğŸ“ˆ 95%ç½®ä¿¡åŒºé—´: [{metrics.confidence_interval_95[0]:.1f}%, {metrics.confidence_interval_95[1]:.1f}%]")
            print(f"   ğŸ’° å¹³å‡å¥–åŠ±: {metrics.avg_reward:+.1f} Â± {metrics.reward_std:.1f}")
            print(f"   ğŸ“ å¹³å‡æ­¥æ•°: {metrics.avg_steps:.1f} Â± {metrics.steps_std:.1f}")
            print(f"   âœ‹ æ¥å®¢æˆåŠŸç‡: {metrics.avg_pickup_rate:.1f}%")
            print(f"   ğŸš— é€å®¢æˆåŠŸç‡: {metrics.avg_dropoff_rate:.1f}%")
            print(f"   ğŸ¯ æ”¶æ•›å›åˆ: {metrics.convergence_episode}")
            print(f"   â±ï¸ æ‰§è¡Œæ—¶é—´: {metrics.execution_time:.2f}s")
            
            # åŠ¨ä½œåˆ†å¸ƒ
            if metrics.decision_distribution:
                total_decisions = sum(metrics.decision_distribution.values())
                print(f"   ğŸ® åŠ¨ä½œåˆ†å¸ƒ:")
                for action, count in sorted(metrics.decision_distribution.items()):
                    percentage = count / total_decisions * 100 if total_decisions > 0 else 0
                    print(f"      {action}: {count} ({percentage:.1f}%)")
    
    def _perform_statistical_analysis(self, sorted_agents):
        """æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ"""
        # è·å–æˆåŠŸç‡æ•°æ®è¿›è¡Œtæ£€éªŒ
        print("   é…å¯¹tæ£€éªŒç»“æœ (på€¼ < 0.05 è¡¨ç¤ºå·®å¼‚æ˜¾è‘—):")
        print("   " + "-" * 45)
        
        agent_names = [name for name, _ in sorted_agents]
        
        # ç®€åŒ–çš„é…å¯¹æ¯”è¾ƒï¼ˆå®é™…åº”è¯¥åŸºäºåŸå§‹æ•°æ®ï¼‰
        for i in range(len(sorted_agents)):
            for j in range(i + 1, len(sorted_agents)):
                name1, metrics1 = sorted_agents[i]
                name2, metrics2 = sorted_agents[j]
                
                # æ¨¡æ‹Ÿtæ£€éªŒï¼ˆå®é™…åº”è¯¥ä½¿ç”¨åŸå§‹æˆåŠŸ/å¤±è´¥æ•°æ®ï¼‰
                diff = abs(metrics1.success_rate - metrics2.success_rate)
                
                # ç®€åŒ–çš„æ˜¾è‘—æ€§åˆ¤æ–­
                if diff > 15:
                    significance = "p < 0.01 ***"
                elif diff > 8:
                    significance = "p < 0.05 **"
                elif diff > 3:
                    significance = "p < 0.1 *"
                else:
                    significance = "p > 0.1 n.s."
                
                print(f"   {name1:<15} vs {name2:<15}: {significance}")
    
    def save_detailed_results(self):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ
        json_filename = f"comprehensive_baseline_results_{timestamp}.json"
        save_data = {
            "experiment_name": "Comprehensive Baseline Comparison",
            "timestamp": datetime.now().isoformat(),
            "config": asdict(self.config),
            "results": {name: asdict(metrics) for name, metrics in self.results.items()}
        }
        
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ ¼å¼çš„æ±‡æ€»ç»“æœ
        csv_filename = f"comprehensive_baseline_summary_{timestamp}.csv"
        with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'Agent', 'Success_Rate', 'Success_Rate_Std', 'Avg_Reward', 'Reward_Std',
                'Avg_Steps', 'Steps_Std', 'Pickup_Rate', 'Dropoff_Rate', 
                'Convergence_Episode', 'Execution_Time', 'CI_Lower', 'CI_Upper'
            ])
            
            for name, metrics in self.results.items():
                writer.writerow([
                    name, metrics.success_rate, metrics.success_rate_std,
                    metrics.avg_reward, metrics.reward_std, metrics.avg_steps, metrics.steps_std,
                    metrics.avg_pickup_rate, metrics.avg_dropoff_rate,
                    metrics.convergence_episode, metrics.execution_time,
                    metrics.confidence_interval_95[0], metrics.confidence_interval_95[1]
                ])
        
        print(f"\nğŸ“ **ç»“æœå·²ä¿å­˜**:")
        print(f"   ğŸ“‹ è¯¦ç»†ç»“æœ: {json_filename}")
        print(f"   ğŸ“Š æ±‡æ€»è¡¨æ ¼: {csv_filename}")

def quick_baseline_test():
    """å¿«é€ŸåŸºçº¿æµ‹è¯•"""
    config = ExperimentConfig(
        episodes=20,
        num_runs=2,
        max_steps_per_episode=200,
        detailed_logging=False,
        clear_libraries=True
    )
    
    experiment = ComprehensiveBaselineExperiment(config)
    experiment.run_complete_comparison()

def standard_baseline_test():
    """æ ‡å‡†åŸºçº¿æµ‹è¯•"""
    config = ExperimentConfig(
        episodes=50,
        num_runs=3,
        max_steps_per_episode=200,
        detailed_logging=False,
        clear_libraries=True,
        statistical_analysis=True
    )
    
    experiment = ComprehensiveBaselineExperiment(config)
    experiment.run_complete_comparison()

def rigorous_baseline_test():
    """ä¸¥æ ¼åŸºçº¿æµ‹è¯•ï¼ˆç”¨äºå­¦æœ¯å‘è¡¨ï¼‰"""
    config = ExperimentConfig(
        episodes=100,
        num_runs=5,
        max_steps_per_episode=200,
        detailed_logging=False,
        clear_libraries=True,
        statistical_analysis=True,
        save_individual_results=True
    )
    
    experiment = ComprehensiveBaselineExperiment(config)
    experiment.run_complete_comparison()

def optimal_baseline_test():
    """æœ€ä¼˜åŒ–åŸºçº¿æµ‹è¯• - åŸºäºå­¦ä¹ é¥±å’Œæ•ˆåº”çš„è®ºæ–‡é…ç½®"""
    print("ğŸ¯ **æœ€ä¼˜åŒ–åŸºçº¿å®éªŒ** (åŸºäºå­¦ä¹ é¥±å’Œæ•ˆåº”å‘ç°)")
    print("=" * 60)
    print("ğŸ“Š é…ç½®: 25å›åˆ Ã— 20æ¬¡è¿è¡Œ")
    print("ğŸ¯ ç†è®ºåŸºç¡€: ILAIåœ¨20å›åˆåå‡ºç°æ€§èƒ½é¥±å’Œ")
    print("ğŸ“ˆ é¢„æœŸç»“æœ: æˆåŠŸç‡ ~52% (é¿å…é¥±å’Œæ•ˆåº”)")
    print("ğŸ”¬ ç»Ÿè®¡æ ·æœ¬: æ€»è®¡500ä¸ªepisode per agent")
    print("=" * 60)
    
    config = ExperimentConfig(
        episodes=25,                    # æœ€ä¼˜å­¦ä¹ çª—å£ (é¿å…é¥±å’Œ)
        num_runs=20,                    # è®ºæ–‡æ ‡å‡†ç»Ÿè®¡æ ·æœ¬
        max_steps_per_episode=200,
        detailed_logging=True,          # å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•
        clear_libraries=True,           # ç¡®ä¿å®éªŒç‹¬ç«‹æ€§
        statistical_analysis=True,      # å®Œæ•´ç»Ÿè®¡åˆ†æ
        save_individual_results=True    # ä¿å­˜è¯¦ç»†ç»“æœç”¨äºè®ºæ–‡
    )
    
    experiment = ComprehensiveBaselineExperiment(config)
    results = experiment.run_complete_comparison()
    
    print(f"\nğŸ‰ **æœ€ä¼˜åŒ–å®éªŒå®Œæˆ**")
    print(f"ğŸ“Š æ•°æ®æ”¶é›†: {25 * 20 * 6} ä¸ªepisode")
    print(f"ğŸ“ˆ ILAIé¢„æœŸæ€§èƒ½: ~52% (åŸºäºæœ€ä¼˜å­¦ä¹ çª—å£)")
    print(f"ğŸ’¾ è®ºæ–‡çº§ç»“æœå·²ä¿å­˜")
    
    return results

def validation_test():
    """å¿«é€ŸéªŒè¯æœ€ä¼˜é…ç½®"""
    print("ğŸ”¬ **å¿«é€ŸéªŒè¯** (ç¡®è®¤25å›åˆé…ç½®æœ‰æ•ˆæ€§)")
    print("=" * 50)
    
    config = ExperimentConfig(
        episodes=25,
        num_runs=5,                     # å°è§„æ¨¡å¿«é€ŸéªŒè¯
        max_steps_per_episode=200,
        detailed_logging=True,          # è§‚å¯Ÿå­¦ä¹ è¿‡ç¨‹
        clear_libraries=True,
        statistical_analysis=True
    )
    
    experiment = ComprehensiveBaselineExperiment(config)
    results = experiment.run_complete_comparison()
    
    return results

def single_detailed_test():
    """å•æ¬¡è¯¦ç»†æ—¥å¿—å®éªŒ"""
    print("ğŸ” **å•æ¬¡è¯¦ç»†æ—¥å¿—å®éªŒ**")
    print("=" * 50)
    
    config = ExperimentConfig(
        episodes=25,                    # 25å›åˆæµ‹è¯•
        num_runs=1,                     # å•æ¬¡è¿è¡Œ
        max_steps_per_episode=200,
        detailed_logging=True,          # å¯ç”¨è¯¦ç»†æ—¥å¿—
        clear_libraries=True,
        statistical_analysis=False      # å•æ¬¡è¿è¡Œä¸éœ€è¦ç»Ÿè®¡åˆ†æ
    )
    
    experiment = ComprehensiveBaselineExperiment(config)
    
    # æµ‹è¯•æ‰€æœ‰6ä¸ªåŸºçº¿æ™ºèƒ½ä½“
    key_agents = [
        (TaxiILAISystem, "ILAI System", "ilai"),
        (TaxiAStarAgent, "A* Search Agent", "astar"),
        (RealRuleBasedAgent, "Rule-Based Agent", "rule"),
        (TaxiDQNAgent, "Deep Q Network", "dqn"),
        (SimpleQLearningAgent, "Q-Learning Agent", "qlearn"),
        (RealRandomAgent, "Random Agent", "random")
    ]
    
    for agent_class, agent_name, agent_id in key_agents:
        try:
            metrics = experiment.run_agent(agent_class, agent_name, agent_id)
            experiment.results[agent_name] = metrics
            print(f"   âœ… {agent_name} å®Œæˆ: æˆåŠŸç‡ {metrics.success_rate:.1%} Â± {metrics.success_rate_std:.1%}")
        except Exception as e:
            print(f"   âŒ {agent_name} å¤±è´¥: {str(e)}")
            experiment.log(f"é”™è¯¯: {agent_name} - {str(e)}")
    
    # ç”Ÿæˆæœ€ç»ˆæ’è¡Œæ¦œå¹¶å†™å…¥æ—¥å¿—
    if experiment.log_file and experiment.results:
        experiment.log("\n" + "=" * 80)
        experiment.log("ğŸ† **æœ€ç»ˆå®éªŒæ’è¡Œæ¦œ**")
        experiment.log("=" * 80)
        
        # æŒ‰æˆåŠŸç‡æ’åºç”Ÿæˆæ’è¡Œæ¦œ
        sorted_agents = sorted(experiment.results.items(), 
                             key=lambda x: x[1].success_rate if hasattr(x[1], 'success_rate') else 0, 
                             reverse=True)
        
        for rank, (agent_name, metrics) in enumerate(sorted_agents, 1):
            if hasattr(metrics, 'success_rate'):
                experiment.log(f"{rank}. {agent_name}: æˆåŠŸç‡{metrics.success_rate:.1%} Â± {metrics.success_rate_std:.1%}")
                experiment.log(f"   å¹³å‡å¥–åŠ±: {metrics.avg_reward:.1f}, å¹³å‡æ­¥æ•°: {metrics.avg_steps:.1f}")
            else:
                experiment.log(f"{rank}. {agent_name}: æ•°æ®ä¸å¯ç”¨")
        
        experiment.log("=" * 80)
    
    # å…³é—­æ—¥å¿—æ–‡ä»¶
    if experiment.log_file:
        experiment.log("å®éªŒç»“æŸ")
        experiment.log(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        experiment.log_file.close()
    
    return experiment.results

if __name__ == "__main__":
    print("ğŸš€ å…¨é¢åŸºçº¿å¯¹æ¯”å®éªŒ")
    print("1. å¿«é€Ÿæµ‹è¯• (20å›åˆ x 2æ¬¡è¿è¡Œ)")
    print("2. æ ‡å‡†æµ‹è¯• (50å›åˆ x 3æ¬¡è¿è¡Œ)")  
    print("3. ä¸¥æ ¼æµ‹è¯• (100å›åˆ x 5æ¬¡è¿è¡Œ, é€‚åˆå­¦æœ¯å‘è¡¨)")
    print("4. ğŸ¯ æœ€ä¼˜æµ‹è¯• (25å›åˆ x 20æ¬¡è¿è¡Œ, åŸºäºå­¦ä¹ é¥±å’Œæ•ˆåº”)")
    print("5. ğŸ”¬ éªŒè¯æµ‹è¯• (25å›åˆ x 5æ¬¡è¿è¡Œ, å¿«é€Ÿç¡®è®¤)")
    
    choice = input("è¯·é€‰æ‹©å®éªŒçº§åˆ« (1-5): ").strip()
    
    if choice == "1":
        quick_baseline_test()
    elif choice == "2":
        standard_baseline_test()
    elif choice == "3":
        rigorous_baseline_test()
    elif choice == "4":
        optimal_baseline_test()
    elif choice == "5":
        validation_test()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        quick_baseline_test()
