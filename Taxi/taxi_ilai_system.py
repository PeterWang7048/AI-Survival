#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Taxiç¯å¢ƒå®Œæ•´ILAIç³»ç»Ÿ
æ•´åˆE-A-Rç¬¦å·åŒ–ã€åˆ†é˜¶æ®µMRã€BMPã€WBMå’Œäº”åº“ç³»ç»Ÿ
"""

from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import random
import time
import numpy as np

# å¯¼å…¥Taxiä¸“ç”¨ç»„ä»¶
from taxi_ear_symbolizer import TaxiEARSymbolizer, TaxiEARExperience
from taxi_staged_multi_reward import TaxiStagedMultiReward, RewardComponents
from taxi_five_libraries import TaxiFiveLibraries, TaxiDecision
from taxi_simple_bmp_wbm import TaxiSimpleBMP, TaxiPathWBM
# ä¼˜åŒ–å™¨ç›¸å…³å¯¼å…¥å·²ç§»é™¤ - æ¢å¤çº¯å‡€ILAIç³»ç»Ÿ

@dataclass
class TaxiILAIDecisionResult:
    """Taxi ILAIå†³ç­–ç»“æœ"""
    selected_action: str
    confidence: float
    decision_source: str
    reasoning_chain: List[str]
    decision_time: float
    stage: str
    reward_components: Optional[RewardComponents] = None

class TaxiILAISystem:
    """Taxiç¯å¢ƒå®Œæ•´ILAIç³»ç»Ÿ"""
    
    def __init__(self, base_dir: str = "taxi_five_libraries"):
        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self.ear_symbolizer = TaxiEARSymbolizer()
        self.five_libraries = TaxiFiveLibraries(base_dir)
        self.multi_reward = TaxiStagedMultiReward()
        self.bmp = TaxiSimpleBMP(self.five_libraries)
        self.wbm = TaxiPathWBM(self.five_libraries, self.bmp)
        
        # ä¼˜åŒ–å™¨å·²ç§»é™¤ - æ¢å¤çº¯å‡€ILAIç³»ç»Ÿ
        
        # ILAIç³»ç»Ÿé…ç½®
        self.config = {
            # ğŸ”§ ä¼˜åŒ–åçš„æƒé‡é…ç½®ï¼šå¹³è¡¡è§„åˆ™å­¦ä¹ ä¸æ¢ç´¢
            "rule_based_weight": 0.8,      # è¿›ä¸€æ­¥æé«˜è§„åˆ™æƒé‡
            "path_based_weight": 0.0,      # ğŸ›‘ ä¿æŒç¦ç”¨WBMè·¯å¾„è§„åˆ’
            "curiosity_weight": 0.2,       # é™ä½å¥½å¥‡å¿ƒæƒé‡ï¼Œé¿å…è¿‡åº¦æ¢ç´¢
            
            # å­¦ä¹ é…ç½®
            "learning_threshold": 0.6,     # å­¦ä¹ é˜ˆå€¼
            "confidence_threshold": 0.5,   # ç½®ä¿¡åº¦é˜ˆå€¼
            "exploration_rate": 0.2,       # æ¢ç´¢ç‡
            
            # é˜¶æ®µåˆ‡æ¢é…ç½®
            "stage1_focus": "pickup",      # é˜¶æ®µ1ä¸“æ³¨äºæ¥å®¢
            "stage2_focus": "dropoff",     # é˜¶æ®µ2ä¸“æ³¨äºé€å®¢
            
            # åŠ¨ä½œæ˜ å°„
            "action_space": {
                0: "ä¸‹", 1: "ä¸Š", 2: "å³", 3: "å·¦", 4: "pickup", 5: "dropoff"
            }
        }
        
        # å½“å‰çŠ¶æ€è·Ÿè¸ª
        self.current_episode = 0
        self.current_step = 0
        self.current_path = None
        self.last_decision = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            "total_decisions": 0,
            "rule_based_decisions": 0,
            "path_based_decisions": 0,
            "curiosity_based_decisions": 0,
            "random_fallback_decisions": 0,
            "successful_pickups": 0,
            "successful_dropoffs": 0,
            "completed_episodes": 0
        }
        
    def make_decision(self, observation: int, available_actions: List[int] = None) -> TaxiILAIDecisionResult:
        """åšå‡ºæ™ºèƒ½å†³ç­–"""
        start_time = time.time()
        
        # ç¬¦å·åŒ–å½“å‰çŠ¶æ€
        current_environment = self.ear_symbolizer.symbolize_environment(observation)
        current_stage = self.ear_symbolizer.get_current_stage(observation)
        
        # åˆå§‹åŒ–æ¨ç†é“¾
        reasoning_chain = [f"å½“å‰çŠ¶æ€: {current_environment}", f"å½“å‰é˜¶æ®µ: {current_stage}"]
        
        # å¯ç”¨åŠ¨ä½œï¼ˆå¦‚æœæœªæä¾›ï¼Œä½¿ç”¨æ‰€æœ‰åŠ¨ä½œï¼‰
        if available_actions is None:
            available_actions = list(range(6))
        
        available_action_names = [self.config["action_space"][a] for a in available_actions]
        reasoning_chain.append(f"å¯ç”¨åŠ¨ä½œ: {available_action_names}")
        
        # å¤šç­–ç•¥å†³ç­–
        decision_candidates = []
        
        # 1. åŸºäºè§„å¾‹çš„å†³ç­–
        rule_decision = self._make_rule_based_decision(current_environment, current_stage, available_action_names)
        if rule_decision:
            decision_candidates.append({
                "action": rule_decision["action"],
                "confidence": rule_decision["confidence"] * self.config["rule_based_weight"],
                "source": "rule_based",
                "reasoning": rule_decision["reasoning"]
            })
        
        # 2. åŸºäºè·¯å¾„çš„å†³ç­–
        path_decision = self._make_path_based_decision(current_environment, current_stage, available_action_names)
        if path_decision:
            decision_candidates.append({
                "action": path_decision["action"],
                "confidence": path_decision["confidence"] * self.config["path_based_weight"],
                "source": "path_based", 
                "reasoning": path_decision["reasoning"]
            })
        
        # 3. åŸºäºå¥½å¥‡å¿ƒçš„å†³ç­–
        curiosity_decision = self._make_curiosity_based_decision(observation, available_action_names)
        if curiosity_decision:
            decision_candidates.append({
                "action": curiosity_decision["action"],
                "confidence": curiosity_decision["confidence"] * self.config["curiosity_weight"],
                "source": "curiosity_based",
                "reasoning": curiosity_decision["reasoning"]
            })
        
        # 4. éšæœºå…œåº•å†³ç­–
        if not decision_candidates:
            random_action = random.choice(available_action_names)
            decision_candidates.append({
                "action": random_action,
                "confidence": 0.3,
                "source": "random_fallback",
                "reasoning": [f"æ‰€æœ‰ç­–ç•¥å‡å¤±è´¥ï¼Œéšæœºé€‰æ‹©åŠ¨ä½œ: {random_action}"]
            })
        
        # ä¼˜åŒ–å™¨å·²ç§»é™¤ - æ¢å¤ç›´æ¥å†³ç­–æœºåˆ¶
        # é€‰æ‹©æœ€ä½³å†³ç­–
        if decision_candidates:
            best_decision = max(decision_candidates, key=lambda d: d["confidence"])
        else:
            # å…œåº•ï¼šéšæœºé€‰æ‹©
            random_action = random.choice(available_action_names)
            best_decision = {
                "action": random_action,
                "confidence": 0.3,
                "source": "random_fallback",
                "reasoning": [f"æ‰€æœ‰ç­–ç•¥å‡å¤±è´¥ï¼Œéšæœºé€‰æ‹©åŠ¨ä½œ: {random_action}"]
            }
        
        # æ›´æ–°æ¨ç†é“¾
        reasoning_chain.extend(best_decision["reasoning"])
        reasoning_chain.append(f"æœ€ç»ˆé€‰æ‹©: {best_decision['action']} (ç½®ä¿¡åº¦: {best_decision['confidence']:.2f})")
        
        # åˆ›å»ºå†³ç­–ç»“æœ
        decision_time = time.time() - start_time
        
        result = TaxiILAIDecisionResult(
            selected_action=best_decision["action"],
            confidence=best_decision["confidence"],
            decision_source=best_decision["source"],
            reasoning_chain=reasoning_chain,
            decision_time=decision_time,
            stage=current_stage
        )
        
        # è®°å½•å†³ç­–åˆ°äº”åº“
        self._record_decision(result, observation)
        
        # æ›´æ–°ç»Ÿè®¡
        self.performance_stats["total_decisions"] += 1
        self.performance_stats[f"{best_decision['source']}_decisions"] += 1
        
        self.last_decision = result
        return result
    
    def select_action(self, state: int) -> int:
        """å®éªŒè„šæœ¬å…¼å®¹æ¥å£ï¼šé€‰æ‹©åŠ¨ä½œå¹¶è¿”å›æ•´æ•°åŠ¨ä½œID"""
        # åŠ¨ä½œåç§°åˆ°æ•´æ•°çš„æ˜ å°„
        action_name_to_id = {
            "ä¸‹": 0, "ä¸Š": 1, "å³": 2, "å·¦": 3, "pickup": 4, "dropoff": 5
        }
        
        # è°ƒç”¨æ ¸å¿ƒå†³ç­–æ–¹æ³•
        decision_result = self.make_decision(state)
        
        # è½¬æ¢åŠ¨ä½œåç§°ä¸ºæ•´æ•°
        action_name = decision_result.selected_action
        action_id = action_name_to_id.get(action_name, 0)  # é»˜è®¤è¿”å›0ï¼ˆä¸‹ï¼‰
        
        # å­˜å‚¨è¯¦ç»†å†³ç­–ç»“æœä¾›æ—¥å¿—ä½¿ç”¨
        self.last_decision_result = decision_result
        
        return action_id
    
    def learn_from_outcome(self, old_observation: int, action_name: str, 
                          new_observation: int, reward: float, done: bool):
        """ä»ç»“æœä¸­å­¦ä¹ """
        
        # å°†åŠ¨ä½œåè½¬æ¢ä¸ºåŠ¨ä½œç´¢å¼•
        action_index = None
        for idx, name in self.config["action_space"].items():
            if name == action_name:
                action_index = idx
                break
        
        if action_index is None:
            print(f"è­¦å‘Šï¼šæœªçŸ¥åŠ¨ä½œ {action_name}")
            return
        
        # åˆ›å»ºE-A-Rç»éªŒ
        experience = self.ear_symbolizer.create_experience(
            old_observation, action_index, new_observation, reward, done
        )
        
        # ç¡®å®šå­¦ä¹ é˜¶æ®µ
        stage = self.ear_symbolizer.get_current_stage(old_observation)
        
        # æ·»åŠ ç»éªŒåˆ°äº”åº“
        self.five_libraries.add_experience(experience, stage)
        
        # ä¼˜åŒ–å™¨å·²ç§»é™¤ - æ¢å¤ç›´æ¥å­¦ä¹ æœºåˆ¶
        # é€šè¿‡BMPç”Ÿæˆè§„å¾‹
        try:
            generated_rule = self.bmp.generate_rule_from_experience(experience, stage)
            if generated_rule:
                print(f"ğŸŒ¸ BMPç”Ÿæˆè§„å¾‹: {generated_rule.environment_pattern} â†’ {generated_rule.action_pattern} â†’ {generated_rule.result_pattern}")
        except Exception as e:
            print(f"BMPè§„å¾‹ç”Ÿæˆå¤±è´¥: {e}")
        
        # è®¡ç®—å¤šå…ƒå¥–åŠ±
        reward_components = self.multi_reward.calculate_stage_reward(
            experience, old_observation, new_observation
        )
        
        # å¦‚æœæœ‰ä¸Šæ¬¡å†³ç­–è®°å½•ï¼Œæ›´æ–°å…¶å¥–åŠ±ç»„ä»¶
        if self.last_decision:
            self.last_decision.reward_components = reward_components
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        if experience.result == "pickupæˆåŠŸ":
            self.performance_stats["successful_pickups"] += 1
        elif experience.result == "dropoffæˆåŠŸ":
            self.performance_stats["successful_dropoffs"] += 1
        
        # ä¼˜åŒ–å™¨å†³ç­–åå¤„ç†å·²ç§»é™¤
        
        if done:
            self.performance_stats["completed_episodes"] += 1
            self.current_episode += 1
            self.current_step = 0
            self.current_path = None
            # ä¼˜åŒ–å™¨å›åˆé‡ç½®å·²ç§»é™¤
        else:
            self.current_step += 1
    
    def _make_rule_based_decision(self, environment: str, stage: str, available_actions: List[str]) -> Optional[Dict]:
        """åŸºäºè§„å¾‹çš„å†³ç­–"""
        
        # è·å–é€‚ç”¨è§„å¾‹
        applicable_rules = self.bmp.get_applicable_rules(environment, stage)
        
        if not applicable_rules:
            return None
        
        # ç­›é€‰å¯ç”¨åŠ¨ä½œçš„è§„å¾‹
        valid_rules = [rule for rule in applicable_rules 
                      if rule.action_pattern in available_actions]
        
        if not valid_rules:
            return None
        
        # é€‰æ‹©æœ€ä½³è§„å¾‹
        best_rule = max(valid_rules, key=lambda r: r.confidence)
        
        reasoning = [
            f"æ‰¾åˆ°{len(applicable_rules)}æ¡é€‚ç”¨è§„å¾‹",
            f"å…¶ä¸­{len(valid_rules)}æ¡åŠ¨ä½œå¯æ‰§è¡Œ",
            f"é€‰æ‹©æœ€ä½³è§„å¾‹: {best_rule.environment_pattern} â†’ {best_rule.action_pattern} â†’ {best_rule.result_pattern}",
            f"è§„å¾‹ç½®ä¿¡åº¦: {best_rule.confidence:.2f}, æ”¯æŒåº¦: {best_rule.support}"
        ]
        
        return {
            "action": best_rule.action_pattern,
            "confidence": best_rule.confidence,
            "reasoning": reasoning
        }
    
    def _make_path_based_decision(self, environment: str, stage: str, available_actions: List[str]) -> Optional[Dict]:
        """åŸºäºè·¯å¾„çš„å†³ç­–ï¼ˆæ¢å¤åˆ°ä¿®å¤å‰çš„ç®€å•ç‰ˆæœ¬ï¼‰"""
        
        # ç¡®å®šç›®æ ‡ç±»å‹
        target_type = self.config["stage1_focus"] if stage == "stage1" else self.config["stage2_focus"]
        
        # è·å–æˆ–ç”Ÿæˆè·¯å¾„
        if not self.current_path:
            self.current_path = self.wbm.plan_path_to_target(environment, target_type, stage)
            
        if not self.current_path:
            return None
        
        # è·å–ä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆç®€å•ç‰ˆæœ¬ï¼šæ€»æ˜¯è·å–ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼‰
        next_action = self.wbm.get_next_action_from_path(self.current_path, 0)
        
        if not next_action or next_action not in available_actions:
            # è·¯å¾„æ— æ•ˆï¼Œæ¸…é™¤
            self.current_path = None
            return None
        
        reasoning = [
            f"ä½¿ç”¨è·¯å¾„è§„åˆ’: {self.current_path.path_id}",
            f"ç›®æ ‡ç±»å‹: {target_type}",
            f"è·¯å¾„ç½®ä¿¡åº¦: {self.current_path.confidence:.2f}",
            f"é€‰æ‹©åŠ¨ä½œ: {next_action}"
        ]
        
        return {
            "action": next_action,
            "confidence": self.current_path.confidence,
            "reasoning": reasoning
        }
    
    def _make_curiosity_based_decision(self, observation: int, available_actions: List[str]) -> Optional[Dict]:
        """åŸºäºå¥½å¥‡å¿ƒçš„å†³ç­–ï¼ˆæ™ºèƒ½ç‰ˆï¼‰"""
        
        # è·å–å½“å‰çŠ¶æ€ä¿¡æ¯
        current_environment = self.ear_symbolizer.symbolize_environment(observation)
        current_stage = self.ear_symbolizer.get_current_stage(observation)
        
        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„ç»¼åˆå¾—åˆ†ï¼ˆæ–°é¢–æ€§ + å¯å‘å¼ï¼‰
        action_scores = {}
        
        for action_name in available_actions:
            # 1. æ–°é¢–æ€§å¾—åˆ†
            state_action_key = (observation, action_name)
            visit_count = self.multi_reward.action_pair_count.get(state_action_key, 0)
            novelty_score = 1.0 / (1.0 + visit_count)
            
            # 2. å¯å‘å¼å¾—åˆ†
            heuristic_score = self._calculate_action_heuristic(current_environment, action_name, current_stage)
            
            # 3. ç»¼åˆå¾—åˆ†
            total_score = novelty_score * 0.6 + heuristic_score * 0.4
            action_scores[action_name] = {
                'total': total_score,
                'novelty': novelty_score,
                'heuristic': heuristic_score
            }
        
        # é€‰æ‹©ç»¼åˆå¾—åˆ†æœ€é«˜çš„åŠ¨ä½œ
        if not action_scores:
            return None
        
        best_action = max(action_scores.keys(), key=lambda a: action_scores[a]['total'])
        best_score_data = action_scores[best_action]
        
        # åªæœ‰å¾—åˆ†è¶³å¤Ÿé«˜æ‰æ¨è
        if best_score_data['total'] < 0.3:
            return None
        
        # æ„å»ºå¾—åˆ†å±•ç¤ºæ–‡æœ¬
        score_text = [(a, f"{s['total']:.2f}") for a, s in action_scores.items()]
        
        reasoning = [
            f"æ™ºèƒ½å¥½å¥‡å¿ƒå†³ç­–",
            f"ç»¼åˆå¾—åˆ†è¯„ä¼°: {score_text}",
            f"é€‰æ‹©æœ€ä½³åŠ¨ä½œ: {best_action} (å¾—åˆ†: {best_score_data['total']:.2f}, æ–°é¢–æ€§: {best_score_data['novelty']:.2f}, å¯å‘å¼: {best_score_data['heuristic']:.2f})"
        ]
        
        return {
            "action": best_action,
            "confidence": best_score_data['total'],
            "reasoning": reasoning
        }
    
    def _calculate_action_heuristic(self, environment: str, action: str, stage: str) -> float:
        """è®¡ç®—åŠ¨ä½œçš„å¯å‘å¼å¾—åˆ†"""
        heuristic_score = 0.5  # åŸºç¡€å¾—åˆ†
        
        # è§£æç¯å¢ƒä¿¡æ¯
        if "pickup_" in environment:
            parts = environment.split("_")
            if len(parts) >= 8:
                pickup_status = parts[1]  # R, G, Y, B, or InCar
                dropoff_location = parts[3]  # R, G, Y, B
                taxi_position = f"taxi_at_{parts[5]}_{parts[6]}"
                
                # é˜¶æ®µ1ï¼šæœªæ¥å®¢ï¼Œä¼˜å…ˆæ¥å®¢ç›¸å…³åŠ¨ä½œ
                if pickup_status != "InCar":
                    if action == "pickup":
                        heuristic_score = 0.9  # æ¥å®¢åŠ¨ä½œé«˜åˆ†
                    elif action in ["ä¸Š", "ä¸‹", "å·¦", "å³"]:
                        # ç§»åŠ¨åŠ¨ä½œæ ¹æ®æ˜¯å¦é è¿‘æ¥å®¢ç‚¹ç»™åˆ†
                        if "é è¿‘" in environment:  # ç®€åŒ–åˆ¤æ–­
                            heuristic_score = 0.7
                        else:
                            heuristic_score = 0.4
                    else:
                        heuristic_score = 0.2  # dropoffåœ¨æ­¤é˜¶æ®µä¸é€‚åˆ
                
                # é˜¶æ®µ2ï¼šå·²æ¥å®¢ï¼Œä¼˜å…ˆé€å®¢ç›¸å…³åŠ¨ä½œ
                elif pickup_status == "InCar":
                    if action == "dropoff":
                        heuristic_score = 0.9  # é€å®¢åŠ¨ä½œé«˜åˆ†
                    elif action in ["ä¸Š", "ä¸‹", "å·¦", "å³"]:
                        # ç§»åŠ¨åŠ¨ä½œæ ¹æ®æ˜¯å¦é è¿‘ç›®çš„åœ°ç»™åˆ†
                        if "é è¿‘ç›®çš„åœ°" in environment:
                            heuristic_score = 0.8
                        else:
                            heuristic_score = 0.5
                    else:
                        heuristic_score = 0.2  # pickupåœ¨æ­¤é˜¶æ®µä¸é€‚åˆ
        
        # é¿å…æ˜æ˜¾ä¸å¥½çš„åŠ¨ä½œ
        if "æ’å¢™" in environment and action in ["ä¸Š", "ä¸‹", "å·¦", "å³"]:
            heuristic_score = 0.1  # æ’å¢™åŠ¨ä½œä½åˆ†
        
        return heuristic_score
    
    def _record_decision(self, result: TaxiILAIDecisionResult, observation: int):
        """è®°å½•å†³ç­–åˆ°äº”åº“"""
        
        decision_id = f"decision_{self.current_episode}_{self.current_step}_{int(time.time()*1000)%10000}"
        environment = self.ear_symbolizer.symbolize_environment(observation)
        
        decision = TaxiDecision(
            decision_id=decision_id,
            environment=environment,
            action=result.selected_action,
            confidence=result.confidence,
            source=result.decision_source,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            rule_id=None,  # å¯ä»¥åœ¨å…·ä½“ç­–ç•¥ä¸­å¡«å……
            path_id=self.current_path.path_id if self.current_path else None,
            stage=result.stage
        )
        
        self.five_libraries.add_decision(decision)
    
    def explain_decision(self, result: TaxiILAIDecisionResult) -> str:
        """ç”Ÿæˆå†³ç­–è§£é‡Š"""
        explanation = f"ğŸš• Taxi ILAIå†³ç­–è§£é‡Š\n"
        explanation += f"{'='*30}\n"
        explanation += f"ğŸ¯ é€‰æ‹©åŠ¨ä½œ: {result.selected_action}\n"
        explanation += f"ğŸ² å†³ç­–ç½®ä¿¡åº¦: {result.confidence:.2f}\n"
        explanation += f"ğŸ” å†³ç­–æ¥æº: {result.decision_source}\n"
        explanation += f"â­ å½“å‰é˜¶æ®µ: {result.stage}\n"
        explanation += f"â±ï¸ å†³ç­–ç”¨æ—¶: {result.decision_time:.4f}ç§’\n"
        
        explanation += f"\nğŸ§  æ¨ç†è¿‡ç¨‹:\n"
        for i, reason in enumerate(result.reasoning_chain, 1):
            explanation += f"   {i}. {reason}\n"
        
        if result.reward_components:
            explanation += f"\nğŸ’° å¥–åŠ±åˆ†è§£:\n"
            explanation += f"   åŸºç¡€å¥–åŠ±: {result.reward_components.base_reward:.2f}\n"
            explanation += f"   å¯¼èˆªå¥–åŠ±: {result.reward_components.navigation_reward:.2f}\n"
            explanation += f"   ä»»åŠ¡å¥–åŠ±: {result.reward_components.task_reward:.2f}\n"
            explanation += f"   æ•ˆç‡å¥–åŠ±: {result.reward_components.efficiency_reward:.2f}\n"
            explanation += f"   æ¢ç´¢å¥–åŠ±: {result.reward_components.exploration_reward:.2f}\n"
            explanation += f"   æ€»å¥–åŠ±: {result.reward_components.total_reward:.2f}\n"
        
        return explanation
    
    def get_comprehensive_statistics(self) -> Dict:
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        
        # è·å–å„ç»„ä»¶ç»Ÿè®¡
        five_libs_stats = self.five_libraries.get_statistics()
        bmp_stats = self.bmp.get_statistics()
        wbm_stats = self.wbm.get_statistics()
        mr_stats = self.multi_reward.get_stage_statistics()
        symbolizer_stats = self.ear_symbolizer.get_statistics_summary()
        
        # ä¼˜åŒ–å™¨ç»Ÿè®¡å·²ç§»é™¤
        
        # ç»„åˆç»Ÿè®¡
        comprehensive_stats = {
            "performance": self.performance_stats,
            "five_libraries": five_libs_stats,
            "bmp": bmp_stats,
            "wbm": wbm_stats,
            "multi_reward": mr_stats,
            "symbolizer": symbolizer_stats,
            "system_info": {
                "current_episode": self.current_episode,
                "current_step": self.current_step,
                "has_active_path": self.current_path is not None
            }
        }
        
        return comprehensive_stats
    
    # ä¼˜åŒ–å™¨æ‘˜è¦æ–¹æ³•å·²ç§»é™¤ - ç³»ç»Ÿæ¢å¤çº¯å‡€çŠ¶æ€

# æµ‹è¯•å‡½æ•°
def test_taxi_ilai_system():
    """æµ‹è¯•Taxi ILAIç³»ç»Ÿ"""
    print("ğŸš• æµ‹è¯•Taxi ILAIå®Œæ•´ç³»ç»Ÿ")
    print("=" * 35)
    
    # åˆ›å»ºILAIç³»ç»Ÿ
    ilai = TaxiILAISystem("test_taxi_ilai_system")
    ilai.five_libraries.clear_all_data()
    
    # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„å†³ç­–-å­¦ä¹ å¾ªç¯
    print(f"\nğŸ¯ æ¨¡æ‹Ÿå†³ç­–-å­¦ä¹ å¾ªç¯:")
    
    # æµ‹è¯•çŠ¶æ€ï¼šå‡ºç§Ÿè½¦åœ¨(3,1)ï¼Œä¹˜å®¢åœ¨Yç«™ç‚¹ï¼Œç›®çš„åœ°æ˜¯Rç«™ç‚¹
    test_state = 328
    
    # ç¬¬1æ­¥ï¼šåšå†³ç­–
    decision1 = ilai.make_decision(test_state, [0, 1, 2, 3, 4])  # ç§»åŠ¨å’ŒpickupåŠ¨ä½œ
    print(f"\nå†³ç­–1:")
    print(f"   é€‰æ‹©åŠ¨ä½œ: {decision1.selected_action}")
    print(f"   å†³ç­–æ¥æº: {decision1.decision_source}")
    print(f"   ç½®ä¿¡åº¦: {decision1.confidence:.2f}")
    
    # ç¬¬2æ­¥ï¼šå­¦ä¹ ç»“æœ
    # å‡è®¾é€‰æ‹©äº†"ä¸‹"åŠ¨ä½œï¼Œç§»åŠ¨åˆ°æ›´é è¿‘Yç«™ç‚¹
    new_state = 428
    reward = -1  # æ­£å¸¸ç§»åŠ¨å¥–åŠ±
    ilai.learn_from_outcome(test_state, decision1.selected_action, new_state, reward, False)
    
    print(f"\nå­¦ä¹ ç»“æœ:")
    print(f"   ç»éªŒå·²æ·»åŠ åˆ°äº”åº“")
    print(f"   BMPå¯èƒ½ç”Ÿæˆäº†æ–°è§„å¾‹")
    
    # ç¬¬3æ­¥ï¼šå†æ¬¡å†³ç­–ï¼ˆåº”è¯¥èƒ½ä½¿ç”¨å­¦åˆ°çš„è§„å¾‹ï¼‰
    decision2 = ilai.make_decision(new_state, [0, 1, 2, 3, 4])
    print(f"\nå†³ç­–2:")
    print(f"   é€‰æ‹©åŠ¨ä½œ: {decision2.selected_action}")
    print(f"   å†³ç­–æ¥æº: {decision2.decision_source}")
    print(f"   ç½®ä¿¡åº¦: {decision2.confidence:.2f}")
    
    # è¯¦ç»†è§£é‡Š
    print(f"\nğŸ“– å†³ç­–è¯¦ç»†è§£é‡Š:")
    print(ilai.explain_decision(decision2))
    
    # ç³»ç»Ÿç»Ÿè®¡
    stats = ilai.get_comprehensive_statistics()
    print(f"\nğŸ“Š ç³»ç»Ÿç»¼åˆç»Ÿè®¡:")
    print(f"   æ€»å†³ç­–æ•°: {stats['performance']['total_decisions']}")
    print(f"   äº”åº“ç»éªŒæ•°: {stats['five_libraries']['total_experiences']}")
    print(f"   äº”åº“è§„å¾‹æ•°: {stats['five_libraries']['total_rules']}")
    print(f"   BMPç”Ÿæˆè§„å¾‹æ•°: {stats['bmp']['generated_rules']}")
    print(f"   WBMç”Ÿæˆè·¯å¾„æ•°: {stats['wbm']['paths_generated']}")

if __name__ == "__main__":
    test_taxi_ilai_system()
