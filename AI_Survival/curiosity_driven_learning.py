#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Curiosity-Driven Learning (CDL) - å¥½å¥‡å¿ƒé©±åŠ¨å­¦ä¹ æ¨¡å—
å®ç°å¥½å¥‡å¿ƒé©±åŠ¨çš„æ¢ç´¢å’Œå­¦ä¹ æœºåˆ¶
"""

import random
import time
from typing import Dict, List, Any, Optional

class ContextState:
    """ä¸Šä¸‹æ–‡çŠ¶æ€ç±»"""
    def __init__(self, symbolized_scene, agent_internal_state, environmental_factors, social_context, timestamp):
        self.symbolized_scene = symbolized_scene
        self.agent_internal_state = agent_internal_state
        self.environmental_factors = environmental_factors
        self.social_context = social_context
        self.timestamp = timestamp

class CuriosityDrivenLearning:
    """å¥½å¥‡å¿ƒé©±åŠ¨å­¦ä¹ ç³»ç»Ÿ"""
    
    def __init__(self, logger=None):
        self.logger = logger
        self.known_contexts = []
        self.current_curiosity = 0.5
        self.evaluation_count = 0
        self.last_action_type = None
        self.action_history = []
        if logger: 
            logger.log("ğŸ§  CDLç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def evaluate_novelty_and_curiosity(self, context_state):
        """è¯„ä¼°æ–°é¢–æ€§å’Œå¥½å¥‡å¿ƒç¨‹åº¦"""
        self.evaluation_count += 1
        
        # è®¡ç®—åŸºç¡€æ–°é¢–æ€§åˆ†æ•°
        novelty = 0.9 if len(self.known_contexts) < 10 else random.uniform(0.7, 0.95)
        
        # è·å–æ™ºèƒ½ä½“çŠ¶æ€
        agent_state = context_state.agent_internal_state
        food = agent_state.get('food', 100)
        water = agent_state.get('water', 100)
        health = agent_state.get('health', 100)
        
        # è·å–ç¯å¢ƒä¿¡æ¯
        symbolized_scene = context_state.symbolized_scene or []
        has_nearby_plants = any(entity.get('type') in ['Strawberry', 'Mushroom'] for entity in symbolized_scene)
        
        # æ™ºèƒ½å†³ç­–é€»è¾‘
        suggested_action = self._determine_smart_action(novelty, food, water, health, has_nearby_plants)
        
        curiosity_score = novelty * 0.8
        should_explore = novelty > 0.8
        
        # è®°å½•è¡ŒåŠ¨å†å²
        self.action_history.append(suggested_action)
        if len(self.action_history) > 20:
            self.action_history = self.action_history[-20:]
        
        result = {
            "novelty_score": novelty,
            "curiosity_score": curiosity_score,
            "average_curiosity": 0.5,
            "should_explore": should_explore,
            "suggested_action": suggested_action
        }
        
        if self.logger:
            self.logger.log(f"ğŸ” CDLè¯„ä¼°#{self.evaluation_count}: æ–°é¢–æ€§={novelty:.2f}, å»ºè®®={suggested_action} (é£Ÿç‰©={food}, æ°´={water}, é™„è¿‘æ¤ç‰©={has_nearby_plants})")
        
        return result
    
    def _determine_smart_action(self, novelty, food, water, health, has_nearby_plants):
        """æ™ºèƒ½å†³å®šå»ºè®®è¡ŒåŠ¨"""
        
        # 1. å¦‚æœèµ„æºä¸è¶³ï¼Œä¼˜å…ˆä¿¡æ¯æ”¶é›†ï¼ˆåŒ…æ‹¬æ”»å‡»åŠ¨ç‰©è·å–é£Ÿç‰©ï¼‰
        if food < 80 or water < 80:
            if random.random() < 0.7:  # 70%æ¦‚ç‡é€‰æ‹©ä¿¡æ¯æ”¶é›†
                return "collect_information"
        
        # 2. å¦‚æœé£Ÿç‰©ä¸¥é‡ä¸è¶³ï¼Œå¼ºåˆ¶å»ºè®®æ”»å‡»è¡Œä¸º
        if food < 50:
            if random.random() < 0.8:  # 80%æ¦‚ç‡å»ºè®®æ”»å‡»
                return "collect_information"  # é€šè¿‡ä¿¡æ¯æ”¶é›†æ‰§è¡Œæ”»å‡»
        
        # 3. å¦‚æœé™„è¿‘æœ‰æ¤ç‰©ï¼Œæœ‰æœºä¼šè¿›è¡Œä¿¡æ¯æ”¶é›†
        if has_nearby_plants and random.random() < 0.6:  # 60%æ¦‚ç‡
            return "collect_information"
        
        # 4. é¿å…è¿ç»­ç›¸åŒè¡ŒåŠ¨ï¼Œå¢åŠ å¤šæ ·æ€§
        recent_actions = self.action_history[-5:] if len(self.action_history) >= 5 else self.action_history
        if recent_actions and len(set(recent_actions)) == 1:  # æœ€è¿‘5æ¬¡éƒ½æ˜¯åŒä¸€è¡ŒåŠ¨
            last_action = recent_actions[0]
            if last_action == "explore":
                return "collect_information"
            elif last_action == "collect_information":
                return "explore"
        
        # 5. åŸºäºæ–°é¢–æ€§çš„é»˜è®¤å†³ç­–
        if novelty > 0.9:
            return "novelty_seeking"
        elif novelty > 0.85:
            # åœ¨æ¢ç´¢å’Œä¿¡æ¯æ”¶é›†ä¹‹é—´éšæœºé€‰æ‹©
            return random.choice(["explore", "collect_information"])
        elif novelty > 0.8:
            return "explore"
        else:
            return "collect_information"
    
    def update_context(self, context_state):
        """æ›´æ–°ä¸Šä¸‹æ–‡å†å²"""
        context_str = str(context_state.agent_internal_state) + str(context_state.environmental_factors)
        self.known_contexts.append(context_str)
        if len(self.known_contexts) > 50:
            self.known_contexts = self.known_contexts[-50:]
        
        if self.logger and self.evaluation_count % 10 == 0:  # å‡å°‘æ—¥å¿—é¢‘ç‡
            self.logger.log(f"ğŸ“ CDLä¸Šä¸‹æ–‡æ›´æ–°: å·²çŸ¥ä¸Šä¸‹æ–‡æ•°é‡={len(self.known_contexts)}")
    
    def evaluate_context_novelty(self, context_state):
        """è¯„ä¼°ä¸Šä¸‹æ–‡æ–°é¢–æ€§"""
        return 0.9
    
    def generate_exploration_strategy(self, context_state, novelty_score):
        """ç”Ÿæˆæ¢ç´¢ç­–ç•¥"""
        return {"priority": "high", "action_type": "explore"} 